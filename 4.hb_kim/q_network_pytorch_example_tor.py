# -*- coding: utf-8 -*-
"""q_network_pytorch_example_tor

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yqiAkg2wrCfuo77W7lMZ7gVknwOOF9nG
"""

import os
import time
import gym
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# define actions
actions = ('left', 'down', 'right', 'up')

def print_policy(num_state, max_row, max_col):
    policy = [agent(s).argmax(1)[0].detach().item() for s in range(num_state)]
    policy = np.asarray([actions[act] for act in policy])
    policy = policy.reshape((max_row, max_col))
    print("\n\n".join('\t'.join(line) for line in policy) + "\n")


def one_hot_encoding(x, num_state):
    # torch.zeros() = (행,열)만큼의 0행렬을 만듬
    out_tensor = torch.zeros([1, num_state])
    out_tensor[0][x] = 1
    return out_tensor
    # autograd 패키지 참고.
    # return np.identity(16)[x:x + 1].astype(np.float32)


# class torch.nn.Module = 모든 신경망의 기본모듈, 지정해주지않으면 optim에서 에러 발생
# Network에서 하고 싶은 것은 현재의 입력에 대한 각 행동의 기대값을 예측하는 것
class QNetwork(nn.Module):
    def __init__(self, state_space, action_space):
        super(QNetwork, self).__init__()
        self.state_space = state_space
        self.hidden_size = state_space

        # nn.Linear = torch의 곱연산 함수
        # L1과 L2가 곱해져서 W가 된다?
        self.l1 = nn.Linear(in_features=self.state_space, out_features=self.hidden_size)
        self.l2 = nn.Linear(in_features=self.hidden_size, out_features=action_space)

    def forward(self, x):
        x = one_hot_encoding(x, self.state_space)
        out1 = torch.sigmoid(self.l1(x))
        return self.l2(out1)

# 코랩에서는 v0 파이참에서는 v1사용
env = gym.make('FrozenLake-v0')

# Set Q-learning parameters
lr = .03
num_episodes = 2000
e = 0.1
dis = 0.99

# Input and output size based on the Env
input_size = env.observation_space.n
output_size = env.action_space.n

# weight
# W = torch.rand([input_size, output_size], dtype=torch.float64)
W = tf.Variable(tf.random.uniform([input_size, output_size], 0, 0.01), dtype=tf.float32)

# Make use of cuda
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# init Q-Network
agent = QNetwork(input_size, output_size).to(device)
# torch optimizer
optimizer = optim.SGD(params=agent.parameters(), lr=lr)
# loss
# Smoothloss = 정답라벨과 예측한 값의 차이
criterion = nn.SmoothL1Loss()

print(W)

start_time = time.time()
# rewards per episode
rList = []
for i in range(num_episodes):
    # Reset environment and get first new observation
    state = env.reset()
    rAll = 0
    done = False
    local_loss = []

    e = 1. / ((i / 50) + 10)
    # The Q-Table learning algorithm
    while not done:
        # Choose an action by greedily (with e chance of random action) from the Q-network
        with torch.no_grad():
            # Do a feedforward pass for the current state s to get predicted Q-values
            # for all actions (=> agent(s)) and use the max as action a: max_a Q(s, a)
            action = agent(state).max(1)[1].view(1, 1)  # max(1)[1] returns index of highest value

        q_value = agent(state).max(1)[0].view(1, 1)
        print(type(q_value))
        # if np.random.rand(1) < e:
        #     action = env.action_space.sample()
        # else:
        #     action = np.argmax(q_value)

        # Get new state and reward from environment
        # perform action to get reward r, next state s1 and game_over flag
        # calculate maximum overall network outputs: max_a’ Q(s1, a’).
        state_next, reward, done, _ = env.step(action.tolist()[0][0])

        if done:
            # Update Q, and no q_value+1, since it's action termial state
            # q_value[0, action] = reward
            q_value_next = reward
        else:
            # Obtain the Q_s` values by feeding the new state through our network
            q_value_next = agent(state_next).max(1)[0].view(1, 1)
            with torch.no_grad():
                # Update Q
                q_value_new = reward + dis * q_value_next

        # print(q, target_q)
        # Calculate loss
        loss = criterion(q_value, q_value_new)
        if i % 100 == 0:
            print("loss and reward: ", i, loss, reward)

        # Optimize the model
        # 파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있기 때문에
        # optimizer.zero_grad()를 통해 미분값을 계속 0으로 초기화시켜줘야함
        optimizer.zero_grad()

        # backward에서 W업데이트
        loss.backward()
        optimizer.step()

        rAll += reward
        state = state_next
    rList.append(rAll)

# print("\Average steps per episode: " + str(sum(jList) / num_episodes))
print("\nScore over time: " + str(sum(rList) / num_episodes))
print("\nFinal Q-Network Policy:\n")
print_policy(input_size, 4, 4)
# plt.plot(jList)
# plt.plot(rList)
plt.savefig("j_q_network.png")
plt.show()

k = 3







