{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN클래스제작완료_혹시나.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6wOb4_mVJXN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gym\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(Agent):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self._name_arg = ['greedy', 'noise', 'learning_rate', 'discount']\n",
        "        self._init_setting = [False, False, 0, 1.0]\n",
        "        self.__optimizer = None\n",
        "        self.__input_size = self.env.observation_space.n\n",
        "\n",
        "\n",
        "        self.__h_size = 10\n",
        "\n",
        "\n",
        "        self.__output_size = self.env.action_space.n\n",
        "\n",
        "\n",
        "        self.__weight1 = None\n",
        "        self.__weight2 = None\n",
        "        self.__deque_maxlen = 2000\n",
        "        self.__num_episodes = 20000\n",
        "        self.__random_sample = 50\n",
        "        self.__num_sample = 50\n",
        "        self.__num_choice = 10\n",
        "\n",
        "\n",
        "    def get_weight(self):\n",
        "        return np.array(self.__weight2.numpy()).tolist()\n",
        "\n",
        "    def run(self, num_episodes, q_map = None, early_stopping=False, **kwargs):\n",
        "        del self._log_epi\n",
        "        self._log_epi = []\n",
        "        if early_stopping:\n",
        "            early_stopping.clear()\n",
        "\n",
        "        learning_rate = 0.1\n",
        "\n",
        "\n",
        "        #weight\n",
        "        self.__weight1 = tf.Variable(tf.random.uniform([self.__input_size, self.__h_size], 0, 0.01), dtype=tf.float32)\n",
        "        self.__weight2 = tf.Variable(tf.random.uniform([self.__h_size, self.__output_size], 0, 0.01), dtype=tf.float32)\n",
        "\n",
        "\n",
        "        #optimizer\n",
        "        self.__optimizer = optimizer = tf.optimizers.SGD(learning_rate=learning_rate)\n",
        "        \n",
        "\n",
        "        start_time = time.time()\n",
        "        q_map = self._load_map(q_map)\n",
        "        for idx in range(num_episodes):\n",
        "            self._run_episodes(q_map, setting=kwargs)\n",
        "\n",
        "            num_ = self._print_progress(idx, num_episodes)\n",
        "            if self._check_early_stopping(early_stopping):\n",
        "                print(f'progress = {num_} %  --> {idx}/{num_episodes} Early Stopping')\n",
        "                break\n",
        "        sum_reward_by_epi = self._get_log_sum_reward()\n",
        "        print(f'{(time.time() - start_time)} seconds')\n",
        "        return q_map, sum_reward_by_epi\n",
        "\n",
        "    # def __run_episodes(self, q_map, idx=0, greedy=False, noise=False, learning_rate=0, discount=1.):\n",
        "    def _run_episodes(self, q_map, idx=0, setting=None):\n",
        "        greedy, noise, learning_rate, discount = self._get_setting(setting)\n",
        "        # 시작 state 설정\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        local_loss = []\n",
        "        rList = []\n",
        "\n",
        "\n",
        "        save_weight = []\n",
        "        buffer = deque(maxlen=self.__deque_maxlen)\n",
        "        \n",
        "\n",
        "        log_step = [[] for _ in range(4)]\n",
        "        while not done:\n",
        "            # Choose an action by greedly (with a chance of random action)\n",
        "            # from the Q-network\n",
        "\n",
        "\n",
        "            #q_value = self.__dense_activation1(state)\n",
        "            #q_value = self.__dense_activation2(state)\n",
        "            #q_value = np.array(q_value.numpy())\n",
        "            #save_weight.append(self.__weight2)\n",
        "            dense_layer_1 = tf.matmul(self.__one_hot(state), self.__weight1)\n",
        "            activation_func_1 = tf.nn.relu(dense_layer_1)\n",
        "            dense_layer_2 = tf.matmul(activation_func_1, self.__weight2)\n",
        "            activation_func_2 = tf.nn.relu(dense_layer_2)\n",
        "            q_value = np.array(activation_func_2.numpy())\n",
        "            save_weight.append(self.__weight2)\n",
        "\n",
        "\n",
        "            # e-greedy, noise\n",
        "            # act = np.argmax(q_value)\n",
        "            act = self._get_action_noise(q_value[0], idx=idx, greedy=greedy, noise=noise)\n",
        "            state_next, reward, done, _ = self.env.step(act)\n",
        "\n",
        "\n",
        "            buffer.append((state, act, reward, state_next, done))\n",
        "\n",
        "#--------------------------------------------------------------------------------------\n",
        "            #buffer가 다 찼을 때, 이후의 random.sample 과정을 실행합니다\n",
        "            if len(buffer) >self.__deque_maxlen:\n",
        "                pass\n",
        "            if self.__num_episodes % self.__random_sample ==1:\n",
        "                for _ in range(self.__num_sample):\n",
        "                    sample = random.sample(buffer, self.__num_choice)\n",
        "\n",
        "\n",
        "                    for state, act, reward, state_next, done in sample:\n",
        "                        if done:\n",
        "                            # Update Q, and no q_value+1, since it's action termial state\n",
        "                            q_value[0, act] = reward\n",
        "                        else:\n",
        "                            # input(1,16) * W1(16,10) -> (1,10)\n",
        "                            next_dense_layer_1 = tf.matmul(self.__one_hot(state_next), self.__weight1)\n",
        "                            activation_func_1 = tf.nn.relu(next_dense_layer_1)\n",
        "\n",
        "                            # input(1,10) * W2(10,4) -> (1,4)\n",
        "                            next_dense_layer_2 = tf.matmul(activation_func_1, self.__weight2)\n",
        "                            activation_func_2 = tf.nn.relu(next_dense_layer_2)\n",
        "\n",
        "                            q_score_next = np.array(activation_func_2.numpy())\n",
        "\n",
        "                            q_value[0, act] = reward + discount * np.max(q_score_next)\n",
        "#--------------------------------------------------------------------------------------\n",
        "\n",
        "                    Qpred_dense_layer_1 = tf.matmul(self.__one_hot(state), self.__weight1)\n",
        "                    Qpred_activation_1 = tf.nn.relu(Qpred_dense_layer_1)\n",
        "\n",
        "                    loss = lambda: tf.reduce_sum(input_tensor=tf.square(q_value - tf.nn.relu(tf.matmul(Qpred_activation_1, W2))))\n",
        "                    self.__optimizer.minimize(loss, var_list=self.__weight)\n",
        "\n",
        "                    state = state_next\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "    def __one_hot(self, x):\n",
        "        return np.identity(self.__input_size)[x:x + 1].astype(np.float32)\n",
        "\n",
        "\n",
        "    # dense_layer_1 + activation_func_1 + dense_layer_2 + activation_func_2\n",
        "    def __dense_activation1(self, state):\n",
        "        dense_layer_1 = tf.matmul(self.__one_hot(state), self.__weight1)\n",
        "        activation_func_1 = tf.nn.relu(dense_layer_1)\n",
        "        return activation_func_1\n",
        "\n",
        "    def __dense_activation2(self, state):\n",
        "        dense_layer_2 = tf.matmul(state, self.__weight2)\n",
        "        activation_func_2 = tf.nn.relu(dense_layer_2)\n",
        "        return activation_func_2"
      ],
      "metadata": {
        "id": "MpgYuFoKWHac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jq590ntKdgZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}