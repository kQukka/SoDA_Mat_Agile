{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_dqn_11_good_2015.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/chrisrryan/AI-Gym_DQN/blob/a9bca0831040be6ab7f41ed003c140f9ca81f173/FrozenLake.py"
      ],
      "metadata": {
        "id": "Wvi9HAEJ8zVU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh9oTYFA6Qxq"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers\n",
        "\n",
        "EPISODES = 1000\n",
        "GAMMA = 0.95            # γ (gamma) The reward discount factor. Normally between 0.90-0.99). Favours shorter-term rewards\n",
        "LEARNING_RATE = 0.005   # α (alpha) The learning rate (0.001). Dictates how much existing best known values are impacted by\n",
        "MEMORY_SIZE = 100000 #1000000\n",
        "BATCH_SIZE = 20\n",
        "EXPLORATION_MAX = 1.0\n",
        "EXPLORATION_MIN = 0.01\n",
        "EXPLORATION_DECAY = 0.995"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "    \n",
        "# Currently, memory growth needs to be the same across GPUs\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "def one_hot(x):\n",
        "    return np.identity(env.observation_space.n)[x:x + 1].astype(np.float32)\n",
        "\n",
        "class DQNSolver:\n",
        "    def __init__(self):\n",
        "        self.observation_space = env.observation_space.n \n",
        "        self.action_space = env.action_space.n\n",
        "        self.epsilon = EXPLORATION_MAX\n",
        "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
        "\n",
        "        self.q_network = self.build_compile() # 큐-네트워크 구성\n",
        "        self.target_network = self.build_compile() #  타깃 큐-네트워크 구성\n",
        "\n",
        "        self.target_model() # 가중치를 적용\n",
        "\n",
        "    def build_compile(self):\n",
        "        model = keras.Sequential()\n",
        "        model.add(layers.Dense(24, input_shape=(env.observation_space.n,), activation=\"relu\"))\n",
        "        model.add(layers.Dense(env.action_space.n, activation=\"linear\"))        \n",
        "        model.compile(loss=\"mse\", optimizer=optimizers.Adam(lr=LEARNING_RATE))\n",
        "        return model\n",
        "\n",
        "    def target_model(self):\n",
        "        self.target_network.set_weights(self.q_network.get_weights()) #타겟 네트워크에 저장\n",
        "\n",
        "    def get_target_weights(self):\n",
        "        return self.target_network.get_weights()\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        if self.epsilon > EXPLORATION_MIN:\n",
        "            self.epsilon *= EXPLORATION_DECAY\n",
        "\n",
        "    def remember(self, state, action, reward, new_state, done):        \n",
        "        self.memory.append((state, action, reward, new_state, done))\n",
        "        if len(self.memory) > MEMORY_SIZE:\n",
        "            self.memory.popleft()\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "        else:\n",
        "            action = np.argmax(self.target_network.predict(one_hot(state)))\n",
        "        return action\n",
        "\n",
        "    def q_update(self, state, action, reward, new_state, done):\n",
        "        if done:\n",
        "            target = reward            \n",
        "        else:\n",
        "            target = reward + GAMMA * np.max(self.target_network.predict(one_hot(new_state)))\n",
        "            \n",
        "        target_vector = self.q_network.predict(one_hot(state))[0]\n",
        "        target_vector[action] = target\n",
        "        self.q_network.fit(one_hot(state), target_vector.reshape(-1, self.action_space), epochs=1, verbose=0)\n",
        "\n",
        "\n",
        "    def experience_replay(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
        "        \n",
        "        for state, action, reward, new_state, done in minibatch:\n",
        "            self.q_update(state, action, reward, new_state, done)   \n",
        "            \n",
        "    \n",
        "    def report(self, episode, steps, reward, action, new_state):\n",
        "        colour = '\\033[92m' if reward > 0 else '\\033[91m'\n",
        "        print(\"Episode: \" + str(episode).rjust(4) + '  ε: {:.3f}'.format(self.epsilon) +\n",
        "              \"  Steps: \" + str(steps).rjust(3) + f'  Reward: {colour}' + f\"{reward:+.1f}\" \n",
        "              + \"  action: \" + str(action).rjust(3) + \"  new_state: \" + str(new_state).rjust(3) +'\\033[0m')\n"
      ],
      "metadata": {
        "id": "arrbLHpm7Zr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_solver = DQNSolver()\n",
        "\n",
        "print(\"\\n\\n\" + '\\033[92m' + \"Begin training OpenAI Gym Frozen Lake\" + '\\033[0m' + \"\\n\")\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        steps += 1\n",
        "        action = dqn_solver.act(state)\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if done and reward < 1 :\n",
        "            reward = -1.0\n",
        "        else:\n",
        "            reward = reward  \n",
        "        dqn_solver.remember(state, action, reward, new_state, done) \n",
        "        #dqn_solver.q_update(state, action, reward, new_state, done) \n",
        "        \n",
        "        state = new_state\n",
        "\n",
        "        if done:\n",
        "            dqn_solver.report(episode, steps, reward, action, new_state)\n",
        "            dqn_solver.epsilon_decay()\n",
        "            dqn_solver.experience_replay()\n",
        "        \n",
        "        steps += 1\n",
        "        if steps> 100 :\n",
        "            break\n",
        "    \n",
        "    if episode%10 == 0:\n",
        "        print('-------- target network update -------')\n",
        "        dqn_solver.target_model()\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnq43XTd7bYj",
        "outputId": "e31c2618-7592-44ee-cbd3-738c585704a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[92mBegin training OpenAI Gym Frozen Lake\u001b[0m\n",
            "\n",
            "Episode:    0  ε: 1.000  Steps:  11  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:    1  ε: 0.995  Steps:   7  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:    2  ε: 0.990  Steps:  13  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:    3  ε: 0.985  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:    4  ε: 0.980  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:    5  ε: 0.975  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:    6  ε: 0.970  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:    7  ε: 0.966  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:    8  ε: 0.961  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:    9  ε: 0.956  Steps:  11  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:   10  ε: 0.951  Steps:   3  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:   11  ε: 0.946  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   12  ε: 0.942  Steps:   3  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   13  ε: 0.937  Steps:  19  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   14  ε: 0.932  Steps:  33  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   15  ε: 0.928  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   16  ε: 0.923  Steps:  29  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   17  ε: 0.918  Steps:   9  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   18  ε: 0.914  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   19  ε: 0.909  Steps:  11  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:   20  ε: 0.905  Steps:  13  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:   21  ε: 0.900  Steps:  17  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   22  ε: 0.896  Steps:  13  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:   23  ε: 0.891  Steps:  11  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   24  ε: 0.887  Steps:  37  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   25  ε: 0.882  Steps:  17  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "Episode:   26  ε: 0.878  Steps:  15  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   27  ε: 0.873  Steps:   9  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   28  ε: 0.869  Steps:  29  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   29  ε: 0.865  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   30  ε: 0.860  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:   31  ε: 0.856  Steps:  23  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   32  ε: 0.852  Steps:  11  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   33  ε: 0.848  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   34  ε: 0.843  Steps:  19  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   35  ε: 0.839  Steps:  11  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   36  ε: 0.835  Steps:  27  Reward: \u001b[91m-1.0  action:   1  new_state:   7\u001b[0m\n",
            "Episode:   37  ε: 0.831  Steps:  19  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:   38  ε: 0.827  Steps:  19  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   39  ε: 0.822  Steps:  27  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   40  ε: 0.818  Steps:  11  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:   41  ε: 0.814  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   42  ε: 0.810  Steps:  13  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   43  ε: 0.806  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   44  ε: 0.802  Steps:  11  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   45  ε: 0.798  Steps:  23  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   46  ε: 0.794  Steps:  51  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   47  ε: 0.790  Steps:  19  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   48  ε: 0.786  Steps:  19  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "Episode:   49  ε: 0.782  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   50  ε: 0.778  Steps:  45  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:   51  ε: 0.774  Steps:  11  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   52  ε: 0.771  Steps:   9  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   53  ε: 0.767  Steps:  17  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   54  ε: 0.763  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   55  ε: 0.759  Steps:  19  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   56  ε: 0.755  Steps:  13  Reward: \u001b[91m-1.0  action:   1  new_state:   7\u001b[0m\n",
            "Episode:   57  ε: 0.751  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   58  ε: 0.748  Steps:   7  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   59  ε: 0.744  Steps:  13  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   60  ε: 0.740  Steps:   9  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:   61  ε: 0.737  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   62  ε: 0.733  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   63  ε: 0.729  Steps:  11  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   64  ε: 0.726  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   65  ε: 0.722  Steps:   3  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   66  ε: 0.718  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   67  ε: 0.715  Steps:  21  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   68  ε: 0.711  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   69  ε: 0.708  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:  11\u001b[0m\n",
            "Episode:   70  ε: 0.704  Steps:  17  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:   71  ε: 0.701  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   72  ε: 0.697  Steps:  13  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   73  ε: 0.694  Steps:   9  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   74  ε: 0.690  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   75  ε: 0.687  Steps:  19  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   76  ε: 0.683  Steps:  19  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   77  ε: 0.680  Steps:  11  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:   78  ε: 0.676  Steps:  25  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:   79  ε: 0.673  Steps:  17  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:   80  ε: 0.670  Steps:  27  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:   81  ε: 0.666  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   82  ε: 0.663  Steps:  39  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "Episode:   83  ε: 0.660  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:   84  ε: 0.656  Steps:  15  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:   85  ε: 0.653  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   86  ε: 0.650  Steps:  15  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   87  ε: 0.647  Steps:  29  Reward: \u001b[91m-1.0  action:   2  new_state:  11\u001b[0m\n",
            "Episode:   88  ε: 0.643  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   89  ε: 0.640  Steps:  27  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   90  ε: 0.637  Steps:  23  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:   91  ε: 0.634  Steps:   9  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:   92  ε: 0.631  Steps:  41  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:   93  ε: 0.627  Steps:  47  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "Episode:   94  ε: 0.624  Steps:  21  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   95  ε: 0.621  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   96  ε: 0.618  Steps:  11  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   97  ε: 0.615  Steps:  21  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:   98  ε: 0.612  Steps:  25  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:   99  ε: 0.609  Steps:  13  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  100  ε: 0.606  Steps:   9  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  101  ε: 0.603  Steps:  13  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  102  ε: 0.600  Steps:   7  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  103  ε: 0.597  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  104  ε: 0.594  Steps:  13  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  105  ε: 0.591  Steps:  23  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  106  ε: 0.588  Steps:  15  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  107  ε: 0.585  Steps:  11  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  108  ε: 0.582  Steps:  25  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  109  ε: 0.579  Steps:  33  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "Episode:  110  ε: 0.576  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  111  ε: 0.573  Steps:  29  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  112  ε: 0.570  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  113  ε: 0.568  Steps:  11  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  114  ε: 0.565  Steps:  43  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "Episode:  115  ε: 0.562  Steps:  15  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  116  ε: 0.559  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  117  ε: 0.556  Steps:   7  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  118  ε: 0.554  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  119  ε: 0.551  Steps:  51  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  120  ε: 0.548  Steps:   9  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  121  ε: 0.545  Steps:  41  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  122  ε: 0.543  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  123  ε: 0.540  Steps:  25  Reward: \u001b[91m-1.0  action:   2  new_state:  11\u001b[0m\n",
            "Episode:  124  ε: 0.537  Steps:  35  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  125  ε: 0.534  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  126  ε: 0.532  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:  11\u001b[0m\n",
            "Episode:  127  ε: 0.529  Steps:  49  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  128  ε: 0.526  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  129  ε: 0.524  Steps:  13  Reward: \u001b[91m-1.0  action:   2  new_state:  11\u001b[0m\n",
            "Episode:  130  ε: 0.521  Steps:  19  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  131  ε: 0.519  Steps:  17  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  132  ε: 0.516  Steps:  57  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  133  ε: 0.513  Steps:  45  Reward: \u001b[91m-1.0  action:   2  new_state:  11\u001b[0m\n",
            "Episode:  134  ε: 0.511  Steps:   7  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  135  ε: 0.508  Steps:  17  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  136  ε: 0.506  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  137  ε: 0.503  Steps:  37  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  138  ε: 0.501  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  139  ε: 0.498  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  140  ε: 0.496  Steps:  37  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  141  ε: 0.493  Steps:  19  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  142  ε: 0.491  Steps:  13  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  143  ε: 0.488  Steps:   9  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  144  ε: 0.486  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  145  ε: 0.483  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  146  ε: 0.481  Steps:  23  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  147  ε: 0.479  Steps:   7  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  148  ε: 0.476  Steps:  15  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  149  ε: 0.474  Steps:  25  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  150  ε: 0.471  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  151  ε: 0.469  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  152  ε: 0.467  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  153  ε: 0.464  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  154  ε: 0.462  Steps:  47  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  155  ε: 0.460  Steps:  17  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  156  ε: 0.458  Steps:  37  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  157  ε: 0.455  Steps:  25  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  158  ε: 0.453  Steps:  67  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  159  ε: 0.451  Steps:  83  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  160  ε: 0.448  Steps:  11  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  161  ε: 0.446  Steps:  17  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  162  ε: 0.444  Steps:  31  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  163  ε: 0.442  Steps:  13  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  164  ε: 0.440  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  165  ε: 0.437  Steps:  51  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  166  ε: 0.435  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  167  ε: 0.433  Steps:  11  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  168  ε: 0.431  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  169  ε: 0.429  Steps:  17  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  170  ε: 0.427  Steps:   7  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  171  ε: 0.424  Steps:  29  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  172  ε: 0.422  Steps:  49  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  173  ε: 0.420  Steps:   5  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  174  ε: 0.418  Steps:  87  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  176  ε: 0.416  Steps:  27  Reward: \u001b[91m-1.0  action:   1  new_state:   7\u001b[0m\n",
            "Episode:  178  ε: 0.414  Steps:  21  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  179  ε: 0.412  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  180  ε: 0.410  Steps:  27  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  181  ε: 0.408  Steps:  37  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  182  ε: 0.406  Steps:  21  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  183  ε: 0.404  Steps:  19  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  184  ε: 0.402  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  185  ε: 0.400  Steps:  41  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  186  ε: 0.398  Steps:  27  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  187  ε: 0.396  Steps:  13  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  188  ε: 0.394  Steps:  79  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  189  ε: 0.392  Steps:  15  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  190  ε: 0.390  Steps:  15  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  191  ε: 0.388  Steps:  15  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  192  ε: 0.386  Steps:  21  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  193  ε: 0.384  Steps:  45  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  195  ε: 0.382  Steps:  17  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  196  ε: 0.380  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  197  ε: 0.378  Steps:   9  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  198  ε: 0.376  Steps:  35  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  199  ε: 0.374  Steps:  41  Reward: \u001b[91m-1.0  action:   2  new_state:  11\u001b[0m\n",
            "Episode:  200  ε: 0.373  Steps:   9  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  201  ε: 0.371  Steps:  17  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  202  ε: 0.369  Steps:  29  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  203  ε: 0.367  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  204  ε: 0.365  Steps:  57  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  205  ε: 0.363  Steps:  35  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "Episode:  206  ε: 0.361  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  207  ε: 0.360  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  208  ε: 0.358  Steps:  59  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  209  ε: 0.356  Steps:  35  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  210  ε: 0.354  Steps:  43  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  211  ε: 0.353  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  212  ε: 0.351  Steps:  43  Reward: \u001b[91m-1.0  action:   1  new_state:   7\u001b[0m\n",
            "Episode:  213  ε: 0.349  Steps:   3  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  214  ε: 0.347  Steps:  11  Reward: \u001b[91m-1.0  action:   1  new_state:   7\u001b[0m\n",
            "Episode:  215  ε: 0.346  Steps:  31  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  216  ε: 0.344  Steps:  55  Reward: \u001b[91m-1.0  action:   1  new_state:   7\u001b[0m\n",
            "Episode:  217  ε: 0.342  Steps:  35  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  218  ε: 0.340  Steps:  37  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  220  ε: 0.339  Steps:  19  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  221  ε: 0.337  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  222  ε: 0.335  Steps:  25  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  223  ε: 0.334  Steps:  35  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  224  ε: 0.332  Steps:  33  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  225  ε: 0.330  Steps:  67  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  226  ε: 0.329  Steps:   9  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  227  ε: 0.327  Steps:  29  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  228  ε: 0.325  Steps:  27  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  229  ε: 0.324  Steps:   3  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  230  ε: 0.322  Steps:  17  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  232  ε: 0.321  Steps:  41  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  233  ε: 0.319  Steps:  99  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  234  ε: 0.317  Steps:  33  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "Episode:  235  ε: 0.316  Steps:   5  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  236  ε: 0.314  Steps:  21  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  238  ε: 0.313  Steps:  67  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  239  ε: 0.311  Steps:  83  Reward: \u001b[92m+1.0  action:   2  new_state:  15\u001b[0m\n",
            "Episode:  240  ε: 0.309  Steps:  71  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  241  ε: 0.308  Steps:  23  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  243  ε: 0.306  Steps:  85  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  244  ε: 0.305  Steps:  95  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  245  ε: 0.303  Steps:  81  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  246  ε: 0.302  Steps:  33  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  247  ε: 0.300  Steps:  71  Reward: \u001b[91m-1.0  action:   2  new_state:   7\u001b[0m\n",
            "Episode:  248  ε: 0.299  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  251  ε: 0.297  Steps:  31  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  252  ε: 0.296  Steps:  23  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  253  ε: 0.294  Steps:  13  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  254  ε: 0.293  Steps:   3  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  255  ε: 0.291  Steps:  77  Reward: \u001b[91m-1.0  action:   1  new_state:   5\u001b[0m\n",
            "Episode:  256  ε: 0.290  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  257  ε: 0.288  Steps:  31  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  258  ε: 0.287  Steps:  23  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  259  ε: 0.286  Steps:  75  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  260  ε: 0.284  Steps:  67  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  261  ε: 0.283  Steps:  25  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  262  ε: 0.281  Steps:   3  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  263  ε: 0.280  Steps:  35  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  264  ε: 0.279  Steps:  83  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  265  ε: 0.277  Steps:  17  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  266  ε: 0.276  Steps:  55  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  267  ε: 0.274  Steps:  21  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  268  ε: 0.273  Steps:  11  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  270  ε: 0.272  Steps:  15  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  271  ε: 0.270  Steps:  39  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  272  ε: 0.269  Steps: 101  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  273  ε: 0.268  Steps:  17  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  274  ε: 0.266  Steps:  53  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  275  ε: 0.265  Steps:  35  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  276  ε: 0.264  Steps:   9  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  277  ε: 0.262  Steps:  41  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  278  ε: 0.261  Steps:  55  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n",
            "Episode:  279  ε: 0.260  Steps:  67  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "Episode:  280  ε: 0.258  Steps:   7  Reward: \u001b[91m-1.0  action:   1  new_state:  12\u001b[0m\n",
            "-------- target network update -------\n",
            "Episode:  281  ε: 0.257  Steps:  27  Reward: \u001b[91m-1.0  action:   3  new_state:   5\u001b[0m\n",
            "Episode:  282  ε: 0.256  Steps:  99  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  284  ε: 0.255  Steps:  83  Reward: \u001b[91m-1.0  action:   0  new_state:  12\u001b[0m\n",
            "Episode:  285  ε: 0.253  Steps:  19  Reward: \u001b[91m-1.0  action:   0  new_state:   5\u001b[0m\n",
            "Episode:  286  ε: 0.252  Steps:   7  Reward: \u001b[91m-1.0  action:   2  new_state:   5\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_solver.model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9aQ5LsISIYS",
        "outputId": "240cd634-77a5-4270-fdd3-57880942b954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 24)                408       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 100       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 508\n",
            "Trainable params: 508\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}