{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_2015_2_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DQN 2015 keras version (FrozenLake 8x8)\n",
        "\n",
        "테스트 \n",
        " + fit : stack을 쌓아서 학습\n",
        " + Replay buffer 사이즈  : 5000\n",
        " + 배치 사이즈 변경 : 20->64 샘플링에 성공이 포함되지않을수 있으므로\n",
        " + 히든 유닛 변경 : 24->64"
      ],
      "metadata": {
        "id": "lE-f4cqA5I_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vh9oTYFA6Qxq"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODES = 1000\n",
        "GAMMA = 0.95           \n",
        "LEARNING_RATE = 0.001 \n",
        "MEMORY_SIZE = 5000  \n",
        "BATCH_SIZE = 64\n",
        "EXPLORATION_MAX = 1.0\n",
        "EXPLORATION_MIN = 0.01\n",
        "EXPLORATION_DECAY = 0.999\n",
        "EXPLORATION_DECAY_BIG = 0.8\n",
        "EXPLORATION_DECAY_SMALL = 0.999 \n",
        "\n",
        "#환경 변수\n",
        "IDX_ACTION_LEFT = 0\n",
        "IDX_ACTION_DOWN = 1\n",
        "IDX_ACTION_RIGHT = 2\n",
        "IDX_ACTION_UP = 3\n",
        "\n",
        "STR_ACTION_UP = 'U'\n",
        "STR_ACTION_DOWN = 'D'\n",
        "STR_ACTION_RIGHT = 'R'\n",
        "STR_ACTION_LEFT = 'L'\n",
        "\n",
        "WALL_UPSIDE = [0,1,2,3,4,5,6,7]\n",
        "WALL_DOWNSIDE = [56,57,58,59,60,61,62,63] \n",
        "WALL_RIGHTSIDE = [7,15,23,31,39,47,55,63]\n",
        "WALL_LEFTSIDE = [0,8,16,24,32,40,48,56]\n",
        "\n",
        "GOAL_STATE = 8*8-1\n",
        "TARGET_UPDATE = 10 # ep 10회 마다 업데이트\n",
        "\n",
        "class DQNSolver:\n",
        "    def __init__(self):\n",
        "        self.observation_space = env.observation_space.n \n",
        "        self.action_space = env.action_space.n\n",
        "        self.epsilon = EXPLORATION_MAX\n",
        "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
        "\n",
        "        self.lastest_score = []\n",
        "        self.episodes = []\n",
        "        self.steps = []\n",
        "\n",
        "        self.model = keras.Sequential()\n",
        "        self.model.add(layers.Dense(64, input_shape=(env.observation_space.n,), activation=\"relu\"))\n",
        "        self.model.add(layers.Dense(env.action_space.n, activation=\"linear\"))\n",
        "        self.model.compile(loss=\"mse\", optimizer=optimizers.Adam(lr=LEARNING_RATE))\n",
        "\n",
        "        self.target_model = keras.Sequential()\n",
        "        self.target_model.add(layers.Dense(64, input_shape=(env.observation_space.n,), activation=\"relu\"))\n",
        "        self.target_model.add(layers.Dense(env.action_space.n, activation=\"linear\"))\n",
        "\n",
        "        # 타깃 모델 초기화\n",
        "        self.update_target_model()\n",
        "\n",
        "    # 타깃 모델을 모델의 가중치로 업데이트\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        if self.epsilon > EXPLORATION_MIN:\n",
        "            self.epsilon *= EXPLORATION_DECAY\n",
        "\n",
        "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.append((state, action, reward, new_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.randint(0, env.action_space.n)\n",
        "        else:\n",
        "            action = np.argmax(self.model.predict(np.identity(env.observation_space.n)[state:state + 1])) # <--- main_model\n",
        "        return action\n",
        "\n",
        "    def q_update(self, state, action, reward, new_state, done):\n",
        "        # 예측값 설정 - 현재 상태에 대한 모델의 큐함수\n",
        "        target_vector = self.model.predict(np.identity(self.observation_space)[state:state + 1])[0] # <---  : main_model\n",
        "\n",
        "        # 목표값 설정 - 다음 상태에 대한 타깃 모델의 큐함수 -y 값설정시 업데이트되는 메인 모델이 아닌 타겟모델에서 가져온다.\n",
        "        if done:\n",
        "            target_vector[action] = reward            \n",
        "        else:\n",
        "            t = self.target_model.predict(np.identity(self.observation_space)[new_state:new_state + 1])\n",
        "            target_vector[action] = reward + GAMMA * np.max(t) # <--- target_model \n",
        "\n",
        "        x = np.identity(self.observation_space)[state:state + 1]\n",
        "        y = target_vector.reshape(-1, self.action_space)\n",
        "        self.model.fit(x, y, epochs=1, verbose=0) \n",
        "\n",
        "    def experience_replay(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)\n",
        "        for state, action, reward, new_state, done in batch:\n",
        "            self.q_update(state, action, reward, new_state, done) \n",
        "\n",
        "    def experience_replay_stacking(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        x_stack = np.empty(0).reshape(0, self.observation_space)\n",
        "        y_stack = np.empty(0).reshape(0, self.action_space)\n",
        "\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)\n",
        "        for state, action, reward, new_state, done in batch:\n",
        "            target_vector = self.model.predict(np.identity(self.observation_space)[state:state + 1])[0] \n",
        "            if done:\n",
        "                target_vector[action] = reward            \n",
        "            else:\n",
        "                t = self.target_model.predict(np.identity(self.observation_space)[new_state:new_state + 1])\n",
        "                target_vector[action] = reward + GAMMA * np.max(t) \n",
        "\n",
        "            x = np.identity(self.observation_space)[state:state + 1]\n",
        "            y = target_vector.reshape(-1, self.action_space)\n",
        "            \n",
        "            x_stack = np.vstack([x_stack, x])\n",
        "            y_stack = np.vstack([y_stack, y])\n",
        "\n",
        "        history = self.model.fit(x_stack, y_stack, epochs=1, verbose=0)\n",
        "        return history\n",
        "\n",
        "    \n",
        "    def report(self, episode, steps, state, elasped_time, lastest_score, reward):\n",
        "\n",
        "        self.steps.append(steps)\n",
        "        self.lastest_score.append(lastest_score)\n",
        "        self.episodes.append(episode)\n",
        "\n",
        "        min = int(elasped_time/60)  \n",
        "        sec = int(elasped_time%60)  \n",
        "\n",
        "        colour = '\\033[92m' if reward > 0 else '\\033[91m'\n",
        "        print(\"episode: \" + str(episode).rjust(4) \n",
        "        + ' ε: {:.3f}'.format(self.epsilon) \n",
        "        + \" steps: \" + str(steps).rjust(3) \n",
        "        + \" state: [\" + str(state).rjust(2) +\"]\"\n",
        "        + ' time: {:02d}'.format(min) \n",
        "        + ':{:02d}'.format(sec) \n",
        "        + \" score: \" + str(lastest_score).rjust(2) \n",
        "        + ' memory:' + str(len(self.memory)).rjust(4) \n",
        "        + f' reward: {colour}' + f\"{reward:+.1f}\" + '\\033[0m')\n",
        "    \n",
        "    #학습 결과 시각화\n",
        "    def display_report(self, b_save=False):\n",
        "        plt.plot(self.episodes, self.lastest_score, label='Success score')\n",
        "        plt.plot(self.episodes, self.steps, label='Steps for a goal')\n",
        "        plt.legend()\n",
        "        plt.xlabel('episodes')\n",
        "        plt.show()\n",
        "    \n",
        "    #Goal에 들어온 Path인지 확인\n",
        "    def find_optimal_path(self):\n",
        "        bfind = False\n",
        "\n",
        "        state_size = self.observation_space\n",
        "        action_size = self.action_space\n",
        "        q_map = np.zeros([state_size, action_size])\n",
        "\n",
        "        for state in range(state_size): \n",
        "            q_value = self.model.predict(np.identity(state_size)[state:state + 1])\n",
        "            q_map[state] = q_value\n",
        "            \n",
        "        optimal_step = 0        \n",
        "        qvalue_table = np.zeros([q_map.shape[0], q_map.shape[1]])   \n",
        "        \n",
        "        for state, q_value in enumerate(q_map):\n",
        "            q_max = np.amax(q_value) \n",
        "            indices = np.nonzero(q_value == q_max)[0]\n",
        "            qvalue_table[state, indices[0]] = 1\n",
        "            state += 1\n",
        "\n",
        "        for state, q_value in enumerate(qvalue_table):\n",
        "            index = q_value.argmax()\n",
        "\n",
        "            if optimal_step == state and optimal_step != GOAL_STATE:          \n",
        "                if index == IDX_ACTION_UP and state not in WALL_UPSIDE:\n",
        "                    optimal_step -= 8\n",
        "                elif index == IDX_ACTION_DOWN and state not in WALL_DOWNSIDE:\n",
        "                    optimal_step += 8\n",
        "                elif index == IDX_ACTION_RIGHT and state not in WALL_RIGHTSIDE:\n",
        "                    optimal_step += 1\n",
        "                elif index == IDX_ACTION_LEFT and state not in WALL_LEFTSIDE:\n",
        "                    optimal_step -= 1\n",
        "                else: \n",
        "                    pass\n",
        "        \n",
        "        if optimal_step == GOAL_STATE:\n",
        "            bfind = True\n",
        "\n",
        "        return bfind\n",
        "        \n",
        "env = gym.make('FrozenLake-v0',  map_name='8x8', is_slippery=False)\n",
        "\n",
        "dqn_solver = DQNSolver()\n",
        "start_time = time.time()\n",
        "lastest_reward = deque(maxlen=100) \n",
        "min_steps = 8*8*2\n",
        "keep_cnt = 0\n",
        "\n",
        "np.random.seed(2) \n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        steps += 1\n",
        "        action = dqn_solver.act(state)\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        #reward = -1.0 if done and reward < 1 else reward  \n",
        "        reward = 0 if done and reward < 1 else reward  \n",
        "        \n",
        "        dqn_solver.remember(state, action, reward, new_state, done) \n",
        "        dqn_solver.q_update(state, action, reward, new_state, done) \n",
        "        state = new_state\n",
        "        if done:\n",
        "            elasped_time = time.time()- start_time\n",
        "            if reward == 1 :\n",
        "                lastest_reward.append(1)\n",
        "            else:\n",
        "                lastest_reward.append(0)\n",
        "\n",
        "            lastest_score = sum(lastest_reward)\n",
        "            \n",
        "            dqn_solver.report(episode, steps, state, elasped_time, lastest_score, reward)\n",
        "            dqn_solver.epsilon_decay()\n",
        "            dqn_solver.experience_replay_stacking()\n",
        "\n",
        "            # 최단거리로 Goal간경우 \n",
        "            if reward == 1 and steps <= min_steps:            \n",
        "                print('------------------->  pre min_steps: ', min_steps, 'steps:', steps)\n",
        "                if min_steps != steps :\n",
        "                    min_steps = steps \n",
        "                    keep_cnt = 1   # 최단거리 step 갱신\n",
        "                    print('update > min_steps: ', min_steps, 'keep_cnt:', keep_cnt)\n",
        "                else:\n",
        "                    keep_cnt += 1  # 유지되는 횟수 늘림\n",
        "                    print('keep   > min_steps: ', min_steps, 'keep_cnt:', keep_cnt)\n",
        "\n",
        "            \n",
        "    #타깃 모델을 모델의 가중치로 업데이트\n",
        "    if episode % TARGET_UPDATE == 0:\n",
        "        dqn_solver.update_target_model()\n",
        " \n",
        "    if lastest_score>1 and keep_cnt>4 and dqn_solver.find_optimal_path():\n",
        "        print( episode, 'early stopping!!')\n",
        "        break "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtAp0DvTTVxK",
        "outputId": "c79a8235-664b-4fdf-eb1c-e247149ea87b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode:    0 ε: 1.000 steps:  26 state: [19] time: 00:04 score:  0 memory:  26 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:    1 ε: 0.999 steps:  16 state: [19] time: 00:06 score:  0 memory:  42 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:    2 ε: 0.998 steps:  34 state: [19] time: 00:11 score:  0 memory:  76 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:    3 ε: 0.997 steps:  23 state: [35] time: 00:20 score:  0 memory:  99 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:    4 ε: 0.996 steps:  11 state: [19] time: 00:27 score:  0 memory: 110 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:    5 ε: 0.995 steps:  92 state: [29] time: 00:46 score:  0 memory: 202 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:    6 ε: 0.994 steps:  36 state: [52] time: 00:57 score:  0 memory: 238 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:    7 ε: 0.993 steps:  18 state: [41] time: 01:05 score:  0 memory: 256 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:    8 ε: 0.992 steps:  13 state: [41] time: 01:13 score:  0 memory: 269 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:    9 ε: 0.991 steps:  13 state: [19] time: 01:20 score:  0 memory: 282 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   10 ε: 0.990 steps:  65 state: [29] time: 01:35 score:  0 memory: 347 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   11 ε: 0.989 steps:  47 state: [41] time: 01:48 score:  0 memory: 394 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   12 ε: 0.988 steps:  14 state: [19] time: 01:56 score:  0 memory: 408 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   13 ε: 0.987 steps:  12 state: [19] time: 02:04 score:  0 memory: 420 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   14 ε: 0.986 steps:  30 state: [29] time: 02:14 score:  0 memory: 450 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   15 ε: 0.985 steps:  15 state: [49] time: 02:22 score:  0 memory: 465 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   16 ε: 0.984 steps:  15 state: [35] time: 02:30 score:  0 memory: 480 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   17 ε: 0.983 steps: 100 state: [ 1] time: 02:49 score:  0 memory: 580 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   18 ε: 0.982 steps:  19 state: [19] time: 02:58 score:  0 memory: 599 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   19 ε: 0.981 steps:  14 state: [19] time: 03:05 score:  0 memory: 613 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   20 ε: 0.980 steps:  36 state: [35] time: 03:16 score:  0 memory: 649 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   21 ε: 0.979 steps:  20 state: [19] time: 03:25 score:  0 memory: 669 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   22 ε: 0.978 steps:  18 state: [29] time: 03:33 score:  0 memory: 687 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   23 ε: 0.977 steps: 100 state: [21] time: 03:53 score:  0 memory: 787 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   24 ε: 0.976 steps:  21 state: [35] time: 04:02 score:  0 memory: 808 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   25 ε: 0.975 steps:  10 state: [19] time: 04:09 score:  0 memory: 818 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   26 ε: 0.974 steps:  48 state: [41] time: 04:22 score:  0 memory: 866 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   27 ε: 0.973 steps:  74 state: [19] time: 04:38 score:  0 memory: 940 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   28 ε: 0.972 steps:  70 state: [19] time: 04:54 score:  0 memory:1010 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   29 ε: 0.971 steps:  91 state: [29] time: 05:13 score:  0 memory:1101 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   30 ε: 0.970 steps:  11 state: [19] time: 05:21 score:  0 memory:1112 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   31 ε: 0.969 steps:  14 state: [19] time: 05:29 score:  0 memory:1126 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   32 ε: 0.968 steps:  30 state: [19] time: 05:38 score:  0 memory:1156 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   33 ε: 0.968 steps:  69 state: [19] time: 05:54 score:  0 memory:1225 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   34 ε: 0.967 steps:  43 state: [42] time: 06:06 score:  0 memory:1268 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   35 ε: 0.966 steps:  35 state: [19] time: 06:16 score:  0 memory:1303 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   36 ε: 0.965 steps:   7 state: [19] time: 06:23 score:  0 memory:1310 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   37 ε: 0.964 steps:  20 state: [42] time: 06:31 score:  0 memory:1330 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   38 ε: 0.963 steps:  23 state: [42] time: 06:40 score:  0 memory:1353 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   39 ε: 0.962 steps:  36 state: [42] time: 06:52 score:  0 memory:1389 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   40 ε: 0.961 steps:  11 state: [42] time: 06:59 score:  0 memory:1400 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   41 ε: 0.960 steps:  32 state: [29] time: 07:09 score:  0 memory:1432 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   42 ε: 0.959 steps:  35 state: [19] time: 07:20 score:  0 memory:1467 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   43 ε: 0.958 steps:   8 state: [41] time: 07:27 score:  0 memory:1475 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   44 ε: 0.957 steps:  15 state: [19] time: 07:35 score:  0 memory:1490 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   45 ε: 0.956 steps:  35 state: [29] time: 07:46 score:  0 memory:1525 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   46 ε: 0.955 steps:  68 state: [19] time: 08:01 score:  0 memory:1593 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   47 ε: 0.954 steps:  29 state: [19] time: 08:11 score:  0 memory:1622 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   48 ε: 0.953 steps:  10 state: [19] time: 08:18 score:  0 memory:1632 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   49 ε: 0.952 steps: 100 state: [12] time: 08:38 score:  0 memory:1732 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   50 ε: 0.951 steps:  17 state: [19] time: 08:46 score:  0 memory:1749 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   51 ε: 0.950 steps:  26 state: [41] time: 08:56 score:  0 memory:1775 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   52 ε: 0.949 steps:  16 state: [35] time: 09:04 score:  0 memory:1791 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   53 ε: 0.948 steps:  66 state: [19] time: 09:19 score:  0 memory:1857 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   54 ε: 0.947 steps:  33 state: [41] time: 09:30 score:  0 memory:1890 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   55 ε: 0.946 steps:  26 state: [19] time: 09:40 score:  0 memory:1916 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   56 ε: 0.946 steps:  11 state: [19] time: 09:47 score:  0 memory:1927 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   57 ε: 0.945 steps:  20 state: [29] time: 09:56 score:  0 memory:1947 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   58 ε: 0.944 steps:  62 state: [63] time: 10:11 score:  1 memory:2009 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  128 steps: 62\n",
            "update > min_steps:  62 keep_cnt: 1\n",
            "episode:   59 ε: 0.943 steps:  30 state: [29] time: 10:21 score:  1 memory:2039 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   60 ε: 0.942 steps:  24 state: [49] time: 10:30 score:  1 memory:2063 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   61 ε: 0.941 steps:  17 state: [41] time: 10:39 score:  1 memory:2080 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   62 ε: 0.940 steps:  23 state: [19] time: 10:48 score:  1 memory:2103 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   63 ε: 0.939 steps:  15 state: [19] time: 10:56 score:  1 memory:2118 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   64 ε: 0.938 steps:  26 state: [41] time: 11:06 score:  1 memory:2144 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   65 ε: 0.937 steps:   6 state: [41] time: 11:12 score:  1 memory:2150 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   66 ε: 0.936 steps:  17 state: [29] time: 11:21 score:  1 memory:2167 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   67 ε: 0.935 steps:  23 state: [19] time: 11:30 score:  1 memory:2190 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   68 ε: 0.934 steps:  56 state: [19] time: 11:44 score:  1 memory:2246 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   69 ε: 0.933 steps:  27 state: [19] time: 11:54 score:  1 memory:2273 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   70 ε: 0.932 steps:  58 state: [35] time: 12:08 score:  1 memory:2331 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   71 ε: 0.931 steps:  31 state: [41] time: 12:18 score:  1 memory:2362 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   72 ε: 0.930 steps:  13 state: [19] time: 12:26 score:  1 memory:2375 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   73 ε: 0.930 steps:  38 state: [41] time: 12:38 score:  1 memory:2413 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   74 ε: 0.929 steps:  15 state: [41] time: 12:46 score:  1 memory:2428 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   75 ε: 0.928 steps:   5 state: [19] time: 12:52 score:  1 memory:2433 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   76 ε: 0.927 steps:  15 state: [19] time: 13:00 score:  1 memory:2448 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   77 ε: 0.926 steps:  19 state: [19] time: 13:09 score:  1 memory:2467 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   78 ε: 0.925 steps:  18 state: [19] time: 13:18 score:  1 memory:2485 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   79 ε: 0.924 steps:  45 state: [19] time: 13:31 score:  1 memory:2530 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   80 ε: 0.923 steps:  31 state: [19] time: 13:41 score:  1 memory:2561 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   81 ε: 0.922 steps:  87 state: [29] time: 13:59 score:  1 memory:2648 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   82 ε: 0.921 steps:  11 state: [49] time: 14:07 score:  1 memory:2659 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   83 ε: 0.920 steps:  24 state: [19] time: 14:16 score:  1 memory:2683 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   84 ε: 0.919 steps:   9 state: [19] time: 14:23 score:  1 memory:2692 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   85 ε: 0.918 steps:  42 state: [29] time: 14:35 score:  1 memory:2734 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   86 ε: 0.918 steps:  12 state: [42] time: 14:43 score:  1 memory:2746 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   87 ε: 0.917 steps:  32 state: [19] time: 14:53 score:  1 memory:2778 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   88 ε: 0.916 steps:  16 state: [19] time: 15:02 score:  1 memory:2794 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   89 ε: 0.915 steps:  14 state: [19] time: 15:09 score:  1 memory:2808 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   90 ε: 0.914 steps:  22 state: [19] time: 15:18 score:  1 memory:2830 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   91 ε: 0.913 steps:  17 state: [19] time: 15:26 score:  1 memory:2847 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   92 ε: 0.912 steps:  25 state: [41] time: 15:36 score:  1 memory:2872 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   93 ε: 0.911 steps: 100 state: [ 5] time: 15:56 score:  1 memory:2972 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   94 ε: 0.910 steps:  32 state: [49] time: 16:07 score:  1 memory:3004 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   95 ε: 0.909 steps:  17 state: [35] time: 16:15 score:  1 memory:3021 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   96 ε: 0.908 steps:  31 state: [52] time: 16:26 score:  1 memory:3052 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   97 ε: 0.908 steps:  42 state: [19] time: 16:38 score:  1 memory:3094 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   98 ε: 0.907 steps:  19 state: [41] time: 16:46 score:  1 memory:3113 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:   99 ε: 0.906 steps:  65 state: [19] time: 17:01 score:  1 memory:3178 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  100 ε: 0.905 steps:  40 state: [49] time: 17:13 score:  1 memory:3218 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  101 ε: 0.904 steps:  25 state: [29] time: 17:23 score:  1 memory:3243 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  102 ε: 0.903 steps:  21 state: [41] time: 17:32 score:  1 memory:3264 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  103 ε: 0.902 steps:  14 state: [19] time: 17:39 score:  1 memory:3278 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  104 ε: 0.901 steps:  55 state: [19] time: 17:53 score:  1 memory:3333 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  105 ε: 0.900 steps:  17 state: [41] time: 18:02 score:  1 memory:3350 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  106 ε: 0.899 steps:  43 state: [41] time: 18:13 score:  1 memory:3393 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  107 ε: 0.898 steps:  22 state: [41] time: 18:22 score:  1 memory:3415 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  108 ε: 0.898 steps:  68 state: [19] time: 18:38 score:  1 memory:3483 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  109 ε: 0.897 steps:  32 state: [35] time: 18:48 score:  1 memory:3515 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  110 ε: 0.896 steps:  67 state: [19] time: 19:03 score:  1 memory:3582 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  111 ε: 0.895 steps:  59 state: [29] time: 19:18 score:  1 memory:3641 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  112 ε: 0.894 steps:   8 state: [19] time: 19:25 score:  1 memory:3649 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  113 ε: 0.893 steps:  66 state: [35] time: 19:41 score:  1 memory:3715 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  114 ε: 0.892 steps:  21 state: [19] time: 19:50 score:  1 memory:3736 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  115 ε: 0.891 steps:  16 state: [49] time: 19:58 score:  1 memory:3752 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  116 ε: 0.890 steps:  40 state: [19] time: 20:09 score:  1 memory:3792 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  117 ε: 0.890 steps:  31 state: [19] time: 20:19 score:  1 memory:3823 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  118 ε: 0.889 steps:  89 state: [19] time: 20:37 score:  1 memory:3912 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  119 ε: 0.888 steps:  96 state: [52] time: 20:57 score:  1 memory:4008 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  120 ε: 0.887 steps:  50 state: [41] time: 21:10 score:  1 memory:4058 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  121 ε: 0.886 steps:  15 state: [42] time: 21:18 score:  1 memory:4073 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  122 ε: 0.885 steps:  73 state: [41] time: 21:34 score:  1 memory:4146 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  123 ε: 0.884 steps:  62 state: [19] time: 21:49 score:  1 memory:4208 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  124 ε: 0.883 steps:  16 state: [41] time: 21:57 score:  1 memory:4224 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  125 ε: 0.882 steps:  11 state: [35] time: 22:04 score:  1 memory:4235 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  126 ε: 0.882 steps:  60 state: [49] time: 22:18 score:  1 memory:4295 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  127 ε: 0.881 steps:  73 state: [29] time: 22:35 score:  1 memory:4368 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  128 ε: 0.880 steps:  22 state: [35] time: 22:44 score:  1 memory:4390 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  129 ε: 0.879 steps:  68 state: [46] time: 23:00 score:  1 memory:4458 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  130 ε: 0.878 steps:  15 state: [41] time: 23:08 score:  1 memory:4473 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  131 ε: 0.877 steps:  12 state: [35] time: 23:15 score:  1 memory:4485 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  132 ε: 0.876 steps:   7 state: [49] time: 23:22 score:  1 memory:4492 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  133 ε: 0.875 steps:  22 state: [19] time: 23:31 score:  1 memory:4514 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  134 ε: 0.875 steps:  90 state: [19] time: 23:50 score:  1 memory:4604 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  135 ε: 0.874 steps:  97 state: [35] time: 24:10 score:  1 memory:4701 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  136 ε: 0.873 steps:  12 state: [19] time: 24:17 score:  1 memory:4713 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  137 ε: 0.872 steps:  11 state: [19] time: 24:24 score:  1 memory:4724 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  138 ε: 0.871 steps:  23 state: [29] time: 24:33 score:  1 memory:4747 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  139 ε: 0.870 steps:  16 state: [35] time: 24:41 score:  1 memory:4763 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  140 ε: 0.869 steps:  11 state: [19] time: 24:48 score:  1 memory:4774 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  141 ε: 0.868 steps:  42 state: [49] time: 25:00 score:  1 memory:4816 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  142 ε: 0.868 steps:  22 state: [41] time: 25:10 score:  1 memory:4838 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  143 ε: 0.867 steps:  96 state: [19] time: 25:29 score:  1 memory:4934 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  144 ε: 0.866 steps:  54 state: [46] time: 25:42 score:  1 memory:4988 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  145 ε: 0.865 steps:  24 state: [19] time: 25:51 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  146 ε: 0.864 steps:  43 state: [41] time: 26:03 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  147 ε: 0.863 steps:  32 state: [49] time: 26:14 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  148 ε: 0.862 steps:  17 state: [35] time: 26:22 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  149 ε: 0.862 steps:  57 state: [19] time: 26:36 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  150 ε: 0.861 steps:   6 state: [19] time: 26:42 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  151 ε: 0.860 steps:  18 state: [19] time: 26:51 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  152 ε: 0.859 steps:  25 state: [19] time: 27:00 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  153 ε: 0.858 steps: 100 state: [ 3] time: 27:20 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  154 ε: 0.857 steps:  33 state: [41] time: 27:31 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  155 ε: 0.856 steps:  62 state: [41] time: 27:46 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  156 ε: 0.855 steps:  43 state: [49] time: 27:57 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  157 ε: 0.855 steps:  35 state: [42] time: 28:08 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  158 ε: 0.854 steps:  25 state: [41] time: 28:18 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  159 ε: 0.853 steps:  25 state: [19] time: 28:27 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  160 ε: 0.852 steps:  20 state: [49] time: 28:36 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  161 ε: 0.851 steps:  37 state: [41] time: 28:47 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  162 ε: 0.850 steps:  24 state: [19] time: 28:56 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  163 ε: 0.850 steps:  22 state: [49] time: 29:05 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  164 ε: 0.849 steps:  34 state: [41] time: 29:16 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  165 ε: 0.848 steps:  64 state: [29] time: 29:31 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  166 ε: 0.847 steps:  12 state: [35] time: 29:39 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  167 ε: 0.846 steps:  46 state: [19] time: 29:51 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  168 ε: 0.845 steps:  21 state: [19] time: 30:00 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  169 ε: 0.844 steps:  14 state: [19] time: 30:08 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  170 ε: 0.844 steps: 100 state: [24] time: 30:28 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  171 ε: 0.843 steps:  60 state: [19] time: 30:43 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  172 ε: 0.842 steps:  12 state: [41] time: 30:51 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  173 ε: 0.841 steps: 100 state: [22] time: 31:11 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  174 ε: 0.840 steps:   5 state: [19] time: 31:18 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  175 ε: 0.839 steps: 100 state: [ 1] time: 31:38 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  176 ε: 0.839 steps:  13 state: [41] time: 31:46 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  177 ε: 0.838 steps:  22 state: [35] time: 31:55 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  178 ε: 0.837 steps:  25 state: [19] time: 32:04 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  179 ε: 0.836 steps:  10 state: [19] time: 32:11 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  180 ε: 0.835 steps:  23 state: [29] time: 32:20 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  181 ε: 0.834 steps:  37 state: [42] time: 32:31 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  182 ε: 0.834 steps:  51 state: [42] time: 32:44 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  183 ε: 0.833 steps:  36 state: [19] time: 32:55 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  184 ε: 0.832 steps:  38 state: [49] time: 33:07 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  185 ε: 0.831 steps:  24 state: [42] time: 33:16 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  186 ε: 0.830 steps:  29 state: [42] time: 33:26 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  187 ε: 0.829 steps:  57 state: [41] time: 33:40 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  188 ε: 0.829 steps:  11 state: [42] time: 33:48 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  189 ε: 0.828 steps:  18 state: [19] time: 33:56 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  190 ε: 0.827 steps:  11 state: [19] time: 34:04 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  191 ε: 0.826 steps:  35 state: [19] time: 34:15 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  192 ε: 0.825 steps:  10 state: [19] time: 34:22 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  193 ε: 0.824 steps:  18 state: [41] time: 34:31 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  194 ε: 0.824 steps:  30 state: [41] time: 34:41 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  195 ε: 0.823 steps:  19 state: [42] time: 34:49 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  196 ε: 0.822 steps:  36 state: [29] time: 35:01 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  197 ε: 0.821 steps:  29 state: [42] time: 35:12 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  198 ε: 0.820 steps:  23 state: [19] time: 35:21 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  199 ε: 0.819 steps:  13 state: [41] time: 35:29 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  200 ε: 0.819 steps:  20 state: [41] time: 35:37 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  201 ε: 0.818 steps:  23 state: [19] time: 35:47 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  202 ε: 0.817 steps:  49 state: [19] time: 36:00 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  203 ε: 0.816 steps:  16 state: [41] time: 36:08 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  204 ε: 0.815 steps:  35 state: [35] time: 36:19 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  205 ε: 0.815 steps:  17 state: [19] time: 36:29 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  206 ε: 0.814 steps:  20 state: [41] time: 36:38 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  207 ε: 0.813 steps:  13 state: [19] time: 36:46 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  208 ε: 0.812 steps:  37 state: [19] time: 36:57 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  209 ε: 0.811 steps:   6 state: [19] time: 37:04 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  210 ε: 0.810 steps:  19 state: [35] time: 37:13 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  211 ε: 0.810 steps:  41 state: [29] time: 37:25 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  212 ε: 0.809 steps:  20 state: [42] time: 37:34 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  213 ε: 0.808 steps:  24 state: [49] time: 37:43 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  214 ε: 0.807 steps:  28 state: [19] time: 37:54 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  215 ε: 0.806 steps:  26 state: [29] time: 38:03 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  216 ε: 0.806 steps:  15 state: [42] time: 38:11 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  217 ε: 0.805 steps:  25 state: [19] time: 38:21 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  218 ε: 0.804 steps:  19 state: [52] time: 38:29 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  219 ε: 0.803 steps: 100 state: [21] time: 38:50 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  220 ε: 0.802 steps:  17 state: [41] time: 38:58 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  221 ε: 0.802 steps:  18 state: [19] time: 39:07 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  222 ε: 0.801 steps:  24 state: [19] time: 39:16 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  223 ε: 0.800 steps:  17 state: [19] time: 39:24 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  224 ε: 0.799 steps:  26 state: [59] time: 39:34 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  225 ε: 0.798 steps:  28 state: [49] time: 39:43 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  226 ε: 0.798 steps:  19 state: [19] time: 39:52 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  227 ε: 0.797 steps:  20 state: [19] time: 40:01 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  228 ε: 0.796 steps:  10 state: [19] time: 40:08 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  229 ε: 0.795 steps:  41 state: [19] time: 40:20 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  230 ε: 0.794 steps:  21 state: [19] time: 40:29 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  231 ε: 0.794 steps:  40 state: [29] time: 40:40 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  232 ε: 0.793 steps:  27 state: [41] time: 40:50 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  233 ε: 0.792 steps:  43 state: [46] time: 41:02 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  234 ε: 0.791 steps:  19 state: [19] time: 41:11 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  235 ε: 0.790 steps: 100 state: [ 5] time: 41:32 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  236 ε: 0.790 steps:   7 state: [19] time: 41:39 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  237 ε: 0.789 steps:  21 state: [19] time: 41:47 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  238 ε: 0.788 steps:  13 state: [35] time: 41:55 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  239 ε: 0.787 steps:  59 state: [35] time: 42:09 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  240 ε: 0.787 steps:  33 state: [41] time: 42:20 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  241 ε: 0.786 steps:  41 state: [19] time: 42:31 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  242 ε: 0.785 steps:  33 state: [19] time: 42:42 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  243 ε: 0.784 steps:  12 state: [29] time: 42:49 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  244 ε: 0.783 steps:  13 state: [19] time: 42:57 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  245 ε: 0.783 steps:   8 state: [19] time: 43:04 score:  0 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  246 ε: 0.782 steps:  73 state: [63] time: 43:20 score:  1 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  247 ε: 0.781 steps:  15 state: [41] time: 43:28 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  248 ε: 0.780 steps:  23 state: [35] time: 43:37 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  249 ε: 0.779 steps:  33 state: [29] time: 43:47 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  250 ε: 0.779 steps:  91 state: [29] time: 44:07 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  251 ε: 0.778 steps:  14 state: [19] time: 44:15 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  252 ε: 0.777 steps:  81 state: [29] time: 44:32 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  253 ε: 0.776 steps:  26 state: [42] time: 44:41 score:  1 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  254 ε: 0.776 steps:  56 state: [63] time: 44:55 score:  2 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  62 steps: 56\n",
            "update > min_steps:  56 keep_cnt: 1\n",
            "episode:  255 ε: 0.775 steps:  18 state: [19] time: 45:04 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  256 ε: 0.774 steps:  17 state: [49] time: 45:12 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  257 ε: 0.773 steps:  11 state: [29] time: 45:19 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  258 ε: 0.772 steps:  97 state: [19] time: 45:39 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  259 ε: 0.772 steps:  23 state: [46] time: 45:49 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  260 ε: 0.771 steps:   8 state: [19] time: 45:55 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  261 ε: 0.770 steps:   9 state: [19] time: 46:03 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  262 ε: 0.769 steps:  83 state: [46] time: 46:21 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  263 ε: 0.769 steps:  72 state: [63] time: 46:37 score:  3 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  264 ε: 0.768 steps:  13 state: [19] time: 46:45 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  265 ε: 0.767 steps:  85 state: [42] time: 47:03 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  266 ε: 0.766 steps:   8 state: [41] time: 47:10 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  267 ε: 0.766 steps:  22 state: [41] time: 47:19 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  268 ε: 0.765 steps:  28 state: [19] time: 47:29 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  269 ε: 0.764 steps:   8 state: [19] time: 47:35 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  270 ε: 0.763 steps:  35 state: [19] time: 47:47 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  271 ε: 0.763 steps:  48 state: [29] time: 47:59 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  272 ε: 0.762 steps:   5 state: [19] time: 48:06 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  273 ε: 0.761 steps:  38 state: [29] time: 48:17 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  274 ε: 0.760 steps: 100 state: [38] time: 48:38 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  275 ε: 0.759 steps:   9 state: [19] time: 48:45 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  276 ε: 0.759 steps: 100 state: [21] time: 49:06 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  277 ε: 0.758 steps:  49 state: [29] time: 49:19 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  278 ε: 0.757 steps:  12 state: [19] time: 49:27 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  279 ε: 0.756 steps:  25 state: [42] time: 49:37 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  280 ε: 0.756 steps:  26 state: [35] time: 49:47 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  281 ε: 0.755 steps:  14 state: [19] time: 49:55 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  282 ε: 0.754 steps:  15 state: [19] time: 50:03 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  283 ε: 0.753 steps:  32 state: [35] time: 50:13 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  284 ε: 0.753 steps:  14 state: [35] time: 50:21 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  285 ε: 0.752 steps:  35 state: [46] time: 50:32 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  286 ε: 0.751 steps:  28 state: [46] time: 50:43 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  287 ε: 0.750 steps:  43 state: [63] time: 50:54 score:  4 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  56 steps: 43\n",
            "update > min_steps:  43 keep_cnt: 1\n",
            "episode:  288 ε: 0.750 steps:  31 state: [19] time: 51:05 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  289 ε: 0.749 steps:  28 state: [46] time: 51:15 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  290 ε: 0.748 steps:  59 state: [29] time: 51:29 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  291 ε: 0.747 steps:  22 state: [29] time: 51:38 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  292 ε: 0.747 steps:  57 state: [19] time: 51:52 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  293 ε: 0.746 steps:  35 state: [41] time: 52:03 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  294 ε: 0.745 steps:  15 state: [19] time: 52:11 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  295 ε: 0.744 steps:  56 state: [19] time: 52:25 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  296 ε: 0.744 steps:  13 state: [19] time: 52:33 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  297 ε: 0.743 steps:  15 state: [19] time: 52:41 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  298 ε: 0.742 steps:  34 state: [19] time: 52:51 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  299 ε: 0.741 steps:   7 state: [19] time: 52:58 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  300 ε: 0.741 steps:  17 state: [19] time: 53:06 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  301 ε: 0.740 steps:  46 state: [19] time: 53:19 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  302 ε: 0.739 steps:  21 state: [42] time: 53:27 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  303 ε: 0.738 steps:  10 state: [19] time: 53:34 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  304 ε: 0.738 steps:  20 state: [19] time: 53:43 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  305 ε: 0.737 steps:  30 state: [19] time: 53:53 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  306 ε: 0.736 steps:  17 state: [19] time: 54:01 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  307 ε: 0.736 steps:  56 state: [46] time: 54:15 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  308 ε: 0.735 steps:  22 state: [59] time: 54:24 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  309 ε: 0.734 steps:  21 state: [19] time: 54:34 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  310 ε: 0.733 steps:  32 state: [46] time: 54:44 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  311 ε: 0.733 steps:  60 state: [19] time: 54:58 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  312 ε: 0.732 steps:  91 state: [19] time: 55:18 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  313 ε: 0.731 steps:  12 state: [42] time: 55:26 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  314 ε: 0.730 steps:  19 state: [19] time: 55:34 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  315 ε: 0.730 steps:  61 state: [19] time: 55:49 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  316 ε: 0.729 steps:  27 state: [41] time: 55:59 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  317 ε: 0.728 steps:  30 state: [41] time: 56:09 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  318 ε: 0.727 steps:  58 state: [35] time: 56:23 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  319 ε: 0.727 steps:  10 state: [19] time: 56:31 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  320 ε: 0.726 steps:   5 state: [19] time: 56:37 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  321 ε: 0.725 steps:  58 state: [19] time: 56:52 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  322 ε: 0.725 steps:  12 state: [19] time: 56:59 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  323 ε: 0.724 steps:  49 state: [19] time: 57:12 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  324 ε: 0.723 steps:  24 state: [19] time: 57:21 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  325 ε: 0.722 steps:  41 state: [41] time: 57:33 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  326 ε: 0.722 steps:  52 state: [19] time: 57:46 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  327 ε: 0.721 steps:  43 state: [19] time: 57:59 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  328 ε: 0.720 steps:  13 state: [19] time: 58:06 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  329 ε: 0.720 steps:   5 state: [19] time: 58:13 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  330 ε: 0.719 steps:  22 state: [19] time: 58:22 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  331 ε: 0.718 steps:  33 state: [19] time: 58:33 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  332 ε: 0.717 steps:   5 state: [19] time: 58:39 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  333 ε: 0.717 steps:  43 state: [19] time: 58:51 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  334 ε: 0.716 steps:  23 state: [46] time: 59:01 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  335 ε: 0.715 steps:  36 state: [63] time: 59:12 score:  5 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  43 steps: 36\n",
            "update > min_steps:  36 keep_cnt: 1\n",
            "episode:  336 ε: 0.715 steps:  27 state: [19] time: 59:21 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  337 ε: 0.714 steps:  10 state: [35] time: 59:28 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  338 ε: 0.713 steps:  70 state: [19] time: 59:45 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  339 ε: 0.712 steps:  38 state: [46] time: 59:56 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  340 ε: 0.712 steps:  32 state: [29] time: 60:07 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  341 ε: 0.711 steps:  64 state: [19] time: 60:23 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  342 ε: 0.710 steps:  53 state: [46] time: 60:37 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  343 ε: 0.710 steps:  31 state: [19] time: 60:47 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  344 ε: 0.709 steps:  31 state: [49] time: 61:00 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  345 ε: 0.708 steps:  50 state: [29] time: 61:13 score:  5 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  346 ε: 0.707 steps:  21 state: [49] time: 61:22 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  347 ε: 0.707 steps:  32 state: [19] time: 61:33 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  348 ε: 0.706 steps:  48 state: [29] time: 61:46 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  349 ε: 0.705 steps:  42 state: [29] time: 61:57 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  350 ε: 0.705 steps:  90 state: [46] time: 62:17 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  351 ε: 0.704 steps:  18 state: [19] time: 62:26 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  352 ε: 0.703 steps:  42 state: [19] time: 62:38 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  353 ε: 0.702 steps:  59 state: [46] time: 62:52 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  354 ε: 0.702 steps:  30 state: [63] time: 63:02 score:  4 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  36 steps: 30\n",
            "update > min_steps:  30 keep_cnt: 1\n",
            "episode:  355 ε: 0.701 steps:  44 state: [29] time: 63:15 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  356 ε: 0.700 steps:  12 state: [19] time: 63:22 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  357 ε: 0.700 steps:  13 state: [19] time: 63:30 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  358 ε: 0.699 steps:  18 state: [35] time: 63:38 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  359 ε: 0.698 steps:  29 state: [19] time: 63:49 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  360 ε: 0.698 steps:  27 state: [29] time: 63:58 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  361 ε: 0.697 steps:  37 state: [35] time: 64:10 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  362 ε: 0.696 steps:  13 state: [29] time: 64:17 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  363 ε: 0.695 steps:  19 state: [41] time: 64:26 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  364 ε: 0.695 steps:  13 state: [19] time: 64:33 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  365 ε: 0.694 steps:  23 state: [35] time: 64:43 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  366 ε: 0.693 steps:  32 state: [46] time: 64:53 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  367 ε: 0.693 steps:  30 state: [29] time: 65:04 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  368 ε: 0.692 steps:  28 state: [35] time: 65:14 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  369 ε: 0.691 steps:   9 state: [19] time: 65:21 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  370 ε: 0.691 steps:  13 state: [19] time: 65:28 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  371 ε: 0.690 steps:  29 state: [35] time: 65:38 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  372 ε: 0.689 steps:  37 state: [29] time: 65:50 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  373 ε: 0.689 steps:  25 state: [42] time: 65:59 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  374 ε: 0.688 steps:  42 state: [46] time: 66:12 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  375 ε: 0.687 steps:  50 state: [35] time: 66:25 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  376 ε: 0.686 steps:   8 state: [41] time: 66:32 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  377 ε: 0.686 steps:  11 state: [19] time: 66:39 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  378 ε: 0.685 steps:  25 state: [19] time: 66:48 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  379 ε: 0.684 steps:  20 state: [29] time: 66:57 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  380 ε: 0.684 steps:  47 state: [54] time: 67:10 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  381 ε: 0.683 steps:  14 state: [42] time: 67:18 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  382 ε: 0.682 steps:  29 state: [19] time: 67:28 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  383 ε: 0.682 steps:  14 state: [19] time: 67:36 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  384 ε: 0.681 steps:  39 state: [54] time: 67:47 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  385 ε: 0.680 steps:  13 state: [29] time: 67:55 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  386 ε: 0.680 steps:  43 state: [52] time: 68:08 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  387 ε: 0.679 steps:  56 state: [49] time: 68:22 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  388 ε: 0.678 steps:  12 state: [19] time: 68:29 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  389 ε: 0.678 steps:  35 state: [42] time: 68:40 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  390 ε: 0.677 steps:  13 state: [19] time: 68:48 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  391 ε: 0.676 steps:  36 state: [29] time: 68:59 score:  2 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  392 ε: 0.676 steps:  23 state: [63] time: 69:08 score:  3 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  30 steps: 23\n",
            "update > min_steps:  23 keep_cnt: 1\n",
            "episode:  393 ε: 0.675 steps:  15 state: [29] time: 69:16 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  394 ε: 0.674 steps:  35 state: [46] time: 69:27 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  395 ε: 0.674 steps:  32 state: [54] time: 69:38 score:  3 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  396 ε: 0.673 steps:  26 state: [63] time: 69:48 score:  4 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  397 ε: 0.672 steps:  40 state: [46] time: 70:01 score:  4 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  398 ε: 0.672 steps:  53 state: [63] time: 70:15 score:  5 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  399 ε: 0.671 steps:  28 state: [63] time: 70:24 score:  6 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  400 ε: 0.670 steps:  32 state: [29] time: 70:35 score:  6 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  401 ε: 0.670 steps:  41 state: [63] time: 70:48 score:  7 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  402 ε: 0.669 steps:  19 state: [63] time: 70:56 score:  8 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  23 steps: 19\n",
            "update > min_steps:  19 keep_cnt: 1\n",
            "episode:  403 ε: 0.668 steps:  13 state: [19] time: 71:04 score:  8 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  404 ε: 0.668 steps:  37 state: [35] time: 71:16 score:  8 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  405 ε: 0.667 steps:  30 state: [46] time: 71:26 score:  8 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  406 ε: 0.666 steps:  30 state: [63] time: 71:37 score:  9 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  407 ε: 0.666 steps:  26 state: [41] time: 71:46 score:  9 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  408 ε: 0.665 steps:   5 state: [19] time: 71:53 score:  9 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  409 ε: 0.664 steps:  17 state: [29] time: 72:01 score:  9 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  410 ε: 0.664 steps:  61 state: [63] time: 72:16 score: 10 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  411 ε: 0.663 steps:  37 state: [63] time: 72:28 score: 11 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  412 ε: 0.662 steps:   9 state: [19] time: 72:35 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  413 ε: 0.662 steps:  10 state: [41] time: 72:42 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  414 ε: 0.661 steps:  24 state: [46] time: 72:51 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  415 ε: 0.660 steps:  78 state: [29] time: 73:09 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  416 ε: 0.660 steps:  11 state: [19] time: 73:17 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  417 ε: 0.659 steps:  37 state: [46] time: 73:28 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  418 ε: 0.658 steps:  37 state: [29] time: 73:40 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  419 ε: 0.658 steps:  30 state: [29] time: 73:50 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  420 ε: 0.657 steps:  14 state: [19] time: 73:57 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  421 ε: 0.656 steps:  51 state: [63] time: 74:11 score: 12 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  422 ε: 0.656 steps:  13 state: [29] time: 74:19 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  423 ε: 0.655 steps:  26 state: [29] time: 74:28 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  424 ε: 0.654 steps:  17 state: [41] time: 74:36 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  425 ε: 0.654 steps:  13 state: [41] time: 74:44 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  426 ε: 0.653 steps:  14 state: [19] time: 74:51 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  427 ε: 0.652 steps:  13 state: [19] time: 74:59 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  428 ε: 0.652 steps:  43 state: [63] time: 75:12 score: 13 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  429 ε: 0.651 steps:   5 state: [19] time: 75:18 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  430 ε: 0.650 steps:  23 state: [59] time: 75:27 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  431 ε: 0.650 steps:  36 state: [19] time: 75:38 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  432 ε: 0.649 steps:  28 state: [29] time: 75:48 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  433 ε: 0.648 steps:  21 state: [41] time: 75:57 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  434 ε: 0.648 steps:  90 state: [46] time: 76:18 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  435 ε: 0.647 steps:  25 state: [19] time: 76:27 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  436 ε: 0.646 steps:   6 state: [19] time: 76:34 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  437 ε: 0.646 steps:   9 state: [19] time: 76:41 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  438 ε: 0.645 steps:  12 state: [29] time: 76:49 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  439 ε: 0.645 steps:  49 state: [54] time: 77:02 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  440 ε: 0.644 steps:  21 state: [19] time: 77:11 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  441 ε: 0.643 steps:  14 state: [29] time: 77:19 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  442 ε: 0.643 steps:  57 state: [46] time: 77:34 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  443 ε: 0.642 steps:  23 state: [19] time: 77:43 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  444 ε: 0.641 steps:  31 state: [29] time: 77:54 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  445 ε: 0.641 steps:  34 state: [35] time: 78:05 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  446 ε: 0.640 steps:  12 state: [19] time: 78:13 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  447 ε: 0.639 steps:  44 state: [46] time: 78:25 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  448 ε: 0.639 steps:  10 state: [19] time: 78:33 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  449 ε: 0.638 steps:  39 state: [42] time: 78:44 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  450 ε: 0.637 steps:  29 state: [29] time: 78:55 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  451 ε: 0.637 steps:  16 state: [19] time: 79:03 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  452 ε: 0.636 steps:  43 state: [19] time: 79:15 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  453 ε: 0.636 steps:  14 state: [19] time: 79:23 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  454 ε: 0.635 steps:   8 state: [19] time: 79:30 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  455 ε: 0.634 steps:  27 state: [19] time: 79:39 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  456 ε: 0.634 steps:  24 state: [35] time: 79:49 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  457 ε: 0.633 steps:  27 state: [19] time: 79:59 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  458 ε: 0.632 steps:  13 state: [46] time: 80:07 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  459 ε: 0.632 steps:   9 state: [19] time: 80:14 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  460 ε: 0.631 steps:  10 state: [41] time: 80:21 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  461 ε: 0.631 steps:  42 state: [54] time: 80:33 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  462 ε: 0.630 steps:  45 state: [54] time: 80:46 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  463 ε: 0.629 steps:  20 state: [19] time: 80:54 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  464 ε: 0.629 steps:   7 state: [19] time: 81:01 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  465 ε: 0.628 steps:  17 state: [19] time: 81:09 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  466 ε: 0.627 steps:  15 state: [46] time: 81:17 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  467 ε: 0.627 steps:  29 state: [29] time: 81:27 score: 11 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  468 ε: 0.626 steps:  27 state: [63] time: 81:37 score: 12 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  469 ε: 0.625 steps:  17 state: [29] time: 81:46 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  470 ε: 0.625 steps:   8 state: [19] time: 81:52 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  471 ε: 0.624 steps:   7 state: [19] time: 81:59 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  472 ε: 0.624 steps:   5 state: [19] time: 82:05 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  473 ε: 0.623 steps:  17 state: [29] time: 82:14 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  474 ε: 0.622 steps:  43 state: [63] time: 82:26 score: 13 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  475 ε: 0.622 steps:  25 state: [19] time: 82:36 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  476 ε: 0.621 steps:  27 state: [54] time: 82:46 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  477 ε: 0.620 steps:  52 state: [29] time: 82:59 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  478 ε: 0.620 steps:   7 state: [19] time: 83:05 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  479 ε: 0.619 steps:  22 state: [63] time: 83:15 score: 14 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  480 ε: 0.619 steps:  23 state: [54] time: 83:24 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  481 ε: 0.618 steps:  17 state: [29] time: 83:32 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  482 ε: 0.617 steps:  15 state: [19] time: 83:40 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  483 ε: 0.617 steps:  24 state: [63] time: 83:49 score: 15 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  484 ε: 0.616 steps:  37 state: [19] time: 84:01 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  485 ε: 0.616 steps:  24 state: [19] time: 84:10 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  486 ε: 0.615 steps:   7 state: [19] time: 84:17 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  487 ε: 0.614 steps:  27 state: [63] time: 84:26 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  488 ε: 0.614 steps:  48 state: [63] time: 84:39 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  489 ε: 0.613 steps:  26 state: [63] time: 84:49 score: 18 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  490 ε: 0.612 steps:  47 state: [29] time: 85:02 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  491 ε: 0.612 steps:  24 state: [46] time: 85:11 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  492 ε: 0.611 steps:  11 state: [19] time: 85:19 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  493 ε: 0.611 steps:  10 state: [35] time: 85:26 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  494 ε: 0.610 steps:  16 state: [29] time: 85:34 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  495 ε: 0.609 steps:  11 state: [19] time: 85:41 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  496 ε: 0.609 steps:  18 state: [63] time: 85:50 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  19 steps: 18\n",
            "update > min_steps:  18 keep_cnt: 1\n",
            "episode:  497 ε: 0.608 steps:  19 state: [19] time: 85:58 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  498 ε: 0.608 steps:  32 state: [63] time: 86:09 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  499 ε: 0.607 steps:  36 state: [54] time: 86:19 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  500 ε: 0.606 steps:  13 state: [35] time: 86:27 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  501 ε: 0.606 steps:  41 state: [63] time: 86:39 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  502 ε: 0.605 steps:  21 state: [29] time: 86:48 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  503 ε: 0.605 steps:  20 state: [54] time: 86:57 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  504 ε: 0.604 steps:  79 state: [46] time: 87:14 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  505 ε: 0.603 steps:  37 state: [63] time: 87:26 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  506 ε: 0.603 steps:  28 state: [63] time: 87:36 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  507 ε: 0.602 steps:   9 state: [35] time: 87:43 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  508 ε: 0.602 steps:  50 state: [63] time: 87:56 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  509 ε: 0.601 steps:   8 state: [19] time: 88:03 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  510 ε: 0.600 steps:  27 state: [29] time: 88:13 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  511 ε: 0.600 steps:  14 state: [46] time: 88:21 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  512 ε: 0.599 steps:  10 state: [19] time: 88:27 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  513 ε: 0.599 steps:  29 state: [46] time: 88:37 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  514 ε: 0.598 steps:  20 state: [35] time: 88:46 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  515 ε: 0.597 steps:  18 state: [63] time: 88:55 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  18 steps: 18\n",
            "keep   > min_steps:  18 keep_cnt: 2\n",
            "episode:  516 ε: 0.597 steps:   8 state: [19] time: 89:01 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  517 ε: 0.596 steps:  20 state: [35] time: 89:10 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  518 ε: 0.596 steps:  25 state: [63] time: 89:19 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  519 ε: 0.595 steps:   5 state: [19] time: 89:25 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  520 ε: 0.594 steps:  40 state: [63] time: 89:37 score: 18 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  521 ε: 0.594 steps:   9 state: [19] time: 89:44 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  522 ε: 0.593 steps:  14 state: [46] time: 89:52 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  523 ε: 0.593 steps:  41 state: [46] time: 90:04 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  524 ε: 0.592 steps:   5 state: [19] time: 90:11 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  525 ε: 0.591 steps:   5 state: [19] time: 90:17 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  526 ε: 0.591 steps:  23 state: [19] time: 90:26 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  527 ε: 0.590 steps:  26 state: [19] time: 90:36 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  528 ε: 0.590 steps:  16 state: [19] time: 90:44 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  529 ε: 0.589 steps:  40 state: [29] time: 90:55 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  530 ε: 0.588 steps:  37 state: [46] time: 91:07 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  531 ε: 0.588 steps:  49 state: [54] time: 91:20 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  532 ε: 0.587 steps:  77 state: [46] time: 91:37 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  533 ε: 0.587 steps:  29 state: [63] time: 91:47 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  534 ε: 0.586 steps:  32 state: [46] time: 91:57 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  535 ε: 0.586 steps:   7 state: [19] time: 92:04 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  536 ε: 0.585 steps:  11 state: [19] time: 92:11 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  537 ε: 0.584 steps:  24 state: [46] time: 92:21 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  538 ε: 0.584 steps:  19 state: [46] time: 92:29 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  539 ε: 0.583 steps:  25 state: [29] time: 92:39 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  540 ε: 0.583 steps:  18 state: [29] time: 92:47 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  541 ε: 0.582 steps:  16 state: [19] time: 92:55 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  542 ε: 0.581 steps:  23 state: [19] time: 93:04 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  543 ε: 0.581 steps:  10 state: [19] time: 93:11 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  544 ε: 0.580 steps:  18 state: [54] time: 93:20 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  545 ε: 0.580 steps:  12 state: [19] time: 93:27 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  546 ε: 0.579 steps:  19 state: [54] time: 93:36 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  547 ε: 0.579 steps:  45 state: [46] time: 93:49 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  548 ε: 0.578 steps:  37 state: [46] time: 94:00 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  549 ε: 0.577 steps:   8 state: [35] time: 94:07 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  550 ε: 0.577 steps:  20 state: [29] time: 94:16 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  551 ε: 0.576 steps:  20 state: [54] time: 94:24 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  552 ε: 0.576 steps:  14 state: [29] time: 94:32 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  553 ε: 0.575 steps:  31 state: [54] time: 94:43 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  554 ε: 0.574 steps:   7 state: [19] time: 94:50 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  555 ε: 0.574 steps:  29 state: [29] time: 95:00 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  556 ε: 0.573 steps:  38 state: [46] time: 95:12 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  557 ε: 0.573 steps:  51 state: [63] time: 95:26 score: 18 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  558 ε: 0.572 steps:  52 state: [54] time: 95:40 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  559 ε: 0.572 steps:  35 state: [42] time: 95:51 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  560 ε: 0.571 steps:  29 state: [46] time: 96:01 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  561 ε: 0.570 steps:  26 state: [19] time: 96:11 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  562 ε: 0.570 steps:  32 state: [63] time: 96:22 score: 19 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  563 ε: 0.569 steps:  20 state: [46] time: 96:31 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  564 ε: 0.569 steps:  42 state: [19] time: 96:43 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  565 ε: 0.568 steps:  12 state: [35] time: 96:50 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  566 ε: 0.568 steps:  17 state: [46] time: 96:58 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  567 ε: 0.567 steps:   7 state: [19] time: 97:05 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  568 ε: 0.566 steps:   8 state: [19] time: 97:12 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  569 ε: 0.566 steps:  11 state: [19] time: 97:20 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  570 ε: 0.565 steps:  58 state: [41] time: 97:34 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  571 ε: 0.565 steps:  60 state: [19] time: 97:50 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  572 ε: 0.564 steps:  64 state: [54] time: 98:05 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  573 ε: 0.564 steps:  51 state: [19] time: 98:18 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  574 ε: 0.563 steps:  15 state: [29] time: 98:26 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  575 ε: 0.563 steps:  11 state: [35] time: 98:33 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  576 ε: 0.562 steps:  36 state: [29] time: 98:45 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  577 ε: 0.561 steps: 100 state: [23] time: 99:06 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  578 ε: 0.561 steps:  42 state: [19] time: 99:18 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  579 ε: 0.560 steps:   9 state: [19] time: 99:25 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  580 ε: 0.560 steps:  18 state: [19] time: 99:33 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  581 ε: 0.559 steps:  20 state: [35] time: 99:42 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  582 ε: 0.559 steps:  14 state: [29] time: 99:50 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  583 ε: 0.558 steps:  22 state: [35] time: 99:59 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  584 ε: 0.558 steps:  17 state: [19] time: 100:07 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  585 ε: 0.557 steps:  18 state: [46] time: 100:15 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  586 ε: 0.556 steps:  11 state: [19] time: 100:23 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  587 ε: 0.556 steps:  14 state: [35] time: 100:30 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  588 ε: 0.555 steps:  25 state: [19] time: 100:40 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  589 ε: 0.555 steps:  12 state: [29] time: 100:47 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  590 ε: 0.554 steps:  12 state: [41] time: 100:55 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  591 ε: 0.554 steps:   9 state: [35] time: 101:02 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  592 ε: 0.553 steps:  32 state: [63] time: 101:12 score: 13 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  593 ε: 0.553 steps:  16 state: [63] time: 101:20 score: 14 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  18 steps: 16\n",
            "update > min_steps:  16 keep_cnt: 1\n",
            "episode:  594 ε: 0.552 steps:  23 state: [63] time: 101:30 score: 15 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  595 ε: 0.551 steps:  29 state: [19] time: 101:40 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  596 ε: 0.551 steps:  46 state: [46] time: 101:53 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  597 ε: 0.550 steps:   8 state: [41] time: 101:59 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  598 ε: 0.550 steps:  29 state: [63] time: 102:10 score: 14 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  599 ε: 0.549 steps:  36 state: [49] time: 102:21 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  600 ε: 0.549 steps:  29 state: [19] time: 102:31 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  601 ε: 0.548 steps:  15 state: [19] time: 102:39 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  602 ε: 0.548 steps:  38 state: [54] time: 102:50 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  603 ε: 0.547 steps:  46 state: [29] time: 103:03 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  604 ε: 0.546 steps:  53 state: [63] time: 103:16 score: 14 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  605 ε: 0.546 steps:  24 state: [63] time: 103:26 score: 14 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  606 ε: 0.545 steps:  44 state: [46] time: 103:38 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  607 ε: 0.545 steps:  21 state: [29] time: 103:47 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  608 ε: 0.544 steps:  31 state: [29] time: 103:57 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  609 ε: 0.544 steps:  20 state: [46] time: 104:06 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  610 ε: 0.543 steps:  36 state: [63] time: 104:17 score: 13 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  611 ε: 0.543 steps:  20 state: [29] time: 104:26 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  612 ε: 0.542 steps:  54 state: [46] time: 104:40 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  613 ε: 0.542 steps:  16 state: [29] time: 104:49 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  614 ε: 0.541 steps:   8 state: [19] time: 104:56 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  615 ε: 0.540 steps:  38 state: [63] time: 105:07 score: 13 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  616 ε: 0.540 steps:  75 state: [29] time: 105:25 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  617 ε: 0.539 steps:  33 state: [41] time: 105:36 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  618 ε: 0.539 steps:  42 state: [63] time: 105:48 score: 13 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  619 ε: 0.538 steps:  57 state: [46] time: 106:03 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  620 ε: 0.538 steps:  38 state: [46] time: 106:14 score: 12 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  621 ε: 0.537 steps:  47 state: [63] time: 106:27 score: 13 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  622 ε: 0.537 steps:  39 state: [41] time: 106:39 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  623 ε: 0.536 steps:  27 state: [35] time: 106:49 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  624 ε: 0.536 steps:  24 state: [29] time: 106:58 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  625 ε: 0.535 steps:  25 state: [63] time: 107:07 score: 14 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  626 ε: 0.535 steps:  15 state: [42] time: 107:15 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  627 ε: 0.534 steps:  22 state: [29] time: 107:25 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  628 ε: 0.533 steps:   9 state: [19] time: 107:32 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  629 ε: 0.533 steps:  30 state: [42] time: 107:43 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  630 ε: 0.532 steps:  12 state: [35] time: 107:50 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  631 ε: 0.532 steps:  23 state: [19] time: 108:00 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  632 ε: 0.531 steps:  31 state: [19] time: 108:10 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  633 ε: 0.531 steps:  45 state: [63] time: 108:23 score: 14 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  634 ε: 0.530 steps:  21 state: [54] time: 108:32 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  635 ε: 0.530 steps:  42 state: [46] time: 108:44 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  636 ε: 0.529 steps:  12 state: [29] time: 108:52 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  637 ε: 0.529 steps:  16 state: [42] time: 109:00 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  638 ε: 0.528 steps:  21 state: [29] time: 109:09 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  639 ε: 0.528 steps:  40 state: [63] time: 109:21 score: 15 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  640 ε: 0.527 steps:  17 state: [29] time: 109:29 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  641 ε: 0.527 steps:  21 state: [46] time: 109:38 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  642 ε: 0.526 steps:  21 state: [35] time: 109:48 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  643 ε: 0.526 steps:  11 state: [19] time: 109:55 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  644 ε: 0.525 steps:  31 state: [29] time: 110:06 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  645 ε: 0.524 steps:  18 state: [42] time: 110:14 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  646 ε: 0.524 steps:  20 state: [41] time: 110:23 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  647 ε: 0.523 steps:  10 state: [29] time: 110:30 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  648 ε: 0.523 steps:  22 state: [35] time: 110:39 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  649 ε: 0.522 steps:  11 state: [42] time: 110:46 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  650 ε: 0.522 steps:  17 state: [42] time: 110:55 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  651 ε: 0.521 steps:  15 state: [29] time: 111:03 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  652 ε: 0.521 steps:  16 state: [63] time: 111:12 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  16 steps: 16\n",
            "keep   > min_steps:  16 keep_cnt: 2\n",
            "episode:  653 ε: 0.520 steps:   6 state: [19] time: 111:18 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  654 ε: 0.520 steps:  20 state: [29] time: 111:27 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  655 ε: 0.519 steps:   6 state: [41] time: 111:33 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  656 ε: 0.519 steps:  14 state: [42] time: 111:41 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  657 ε: 0.518 steps:  15 state: [41] time: 111:49 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  658 ε: 0.518 steps:  10 state: [41] time: 111:56 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  659 ε: 0.517 steps:  21 state: [19] time: 112:05 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  660 ε: 0.517 steps:  35 state: [63] time: 112:17 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  661 ε: 0.516 steps:  21 state: [46] time: 112:25 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  662 ε: 0.516 steps:  49 state: [63] time: 112:39 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  663 ε: 0.515 steps:  35 state: [63] time: 112:50 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  664 ε: 0.515 steps:   7 state: [19] time: 112:57 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  665 ε: 0.514 steps:  26 state: [63] time: 113:07 score: 18 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  666 ε: 0.514 steps:  42 state: [46] time: 113:20 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  667 ε: 0.513 steps:  20 state: [63] time: 113:29 score: 19 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  668 ε: 0.513 steps:  27 state: [63] time: 113:39 score: 20 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  669 ε: 0.512 steps:  38 state: [46] time: 113:51 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  670 ε: 0.512 steps:  32 state: [63] time: 114:02 score: 21 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  671 ε: 0.511 steps:  13 state: [19] time: 114:10 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  672 ε: 0.511 steps:  13 state: [35] time: 114:17 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  673 ε: 0.510 steps:  26 state: [19] time: 114:27 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  674 ε: 0.509 steps:  50 state: [19] time: 114:41 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  675 ε: 0.509 steps:  11 state: [19] time: 114:49 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  676 ε: 0.508 steps:  17 state: [19] time: 114:58 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  677 ε: 0.508 steps:   6 state: [19] time: 115:04 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  678 ε: 0.507 steps:  35 state: [63] time: 115:16 score: 22 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  679 ε: 0.507 steps:  24 state: [46] time: 115:26 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  680 ε: 0.506 steps:  35 state: [54] time: 115:37 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  681 ε: 0.506 steps:  11 state: [35] time: 115:44 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  682 ε: 0.505 steps:  30 state: [63] time: 115:55 score: 23 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  683 ε: 0.505 steps:  52 state: [54] time: 116:09 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  684 ε: 0.504 steps:  68 state: [35] time: 116:26 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  685 ε: 0.504 steps:  10 state: [29] time: 116:33 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  686 ε: 0.503 steps:  11 state: [35] time: 116:40 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  687 ε: 0.503 steps:   5 state: [19] time: 116:47 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  688 ε: 0.502 steps:  26 state: [63] time: 116:57 score: 24 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  689 ε: 0.502 steps:  42 state: [54] time: 117:09 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  690 ε: 0.501 steps:  18 state: [63] time: 117:17 score: 25 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  691 ε: 0.501 steps:  14 state: [19] time: 117:25 score: 25 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  692 ε: 0.500 steps:  11 state: [19] time: 117:32 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  693 ε: 0.500 steps:  15 state: [19] time: 117:40 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  694 ε: 0.499 steps:  25 state: [54] time: 117:50 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  695 ε: 0.499 steps:  12 state: [41] time: 117:58 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  696 ε: 0.498 steps:   8 state: [19] time: 118:05 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  697 ε: 0.498 steps:   8 state: [19] time: 118:12 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  698 ε: 0.497 steps:  22 state: [63] time: 118:21 score: 22 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  699 ε: 0.497 steps:  47 state: [46] time: 118:34 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  700 ε: 0.496 steps:  14 state: [29] time: 118:43 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  701 ε: 0.496 steps:   7 state: [19] time: 118:49 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  702 ε: 0.495 steps:  23 state: [54] time: 118:58 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  703 ε: 0.495 steps:  16 state: [19] time: 119:06 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  704 ε: 0.494 steps:   8 state: [19] time: 119:13 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  705 ε: 0.494 steps:  37 state: [63] time: 119:25 score: 21 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  706 ε: 0.493 steps:  40 state: [46] time: 119:37 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  707 ε: 0.493 steps:  47 state: [46] time: 119:50 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  708 ε: 0.492 steps:  20 state: [29] time: 119:59 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  709 ε: 0.492 steps:  21 state: [63] time: 120:08 score: 22 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  710 ε: 0.491 steps:   5 state: [19] time: 120:14 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  711 ε: 0.491 steps:   5 state: [19] time: 120:21 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  712 ε: 0.490 steps:  20 state: [46] time: 120:29 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  713 ε: 0.490 steps:  34 state: [41] time: 120:41 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  714 ε: 0.490 steps:  13 state: [19] time: 120:48 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  715 ε: 0.489 steps:  13 state: [29] time: 120:56 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  716 ε: 0.489 steps:  18 state: [63] time: 121:05 score: 21 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  717 ε: 0.488 steps:  13 state: [19] time: 121:12 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  718 ε: 0.488 steps:  21 state: [29] time: 121:21 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  719 ε: 0.487 steps:  27 state: [46] time: 121:31 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  720 ε: 0.487 steps:   9 state: [19] time: 121:38 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  721 ε: 0.486 steps:   8 state: [19] time: 121:45 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  722 ε: 0.486 steps:   8 state: [19] time: 121:52 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  723 ε: 0.485 steps:  15 state: [46] time: 122:00 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  724 ε: 0.485 steps:  21 state: [46] time: 122:09 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  725 ε: 0.484 steps:  61 state: [63] time: 122:25 score: 19 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  726 ε: 0.484 steps:  43 state: [63] time: 122:37 score: 20 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  727 ε: 0.483 steps:  26 state: [19] time: 122:47 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  728 ε: 0.483 steps:  36 state: [63] time: 122:59 score: 21 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  729 ε: 0.482 steps:   7 state: [19] time: 123:06 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  730 ε: 0.482 steps:  30 state: [63] time: 123:16 score: 22 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  731 ε: 0.481 steps:  15 state: [46] time: 123:24 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  732 ε: 0.481 steps:  27 state: [63] time: 123:34 score: 23 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  733 ε: 0.480 steps:  18 state: [29] time: 123:42 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  734 ε: 0.480 steps:  11 state: [19] time: 123:50 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  735 ε: 0.479 steps:  37 state: [63] time: 124:01 score: 23 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  736 ε: 0.479 steps:   8 state: [19] time: 124:08 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  737 ε: 0.478 steps:  12 state: [19] time: 124:15 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  738 ε: 0.478 steps:  50 state: [63] time: 124:29 score: 24 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  739 ε: 0.477 steps:  26 state: [63] time: 124:39 score: 24 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  740 ε: 0.477 steps:   9 state: [19] time: 124:46 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  741 ε: 0.476 steps:  10 state: [29] time: 124:53 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  742 ε: 0.476 steps:  11 state: [35] time: 125:01 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  743 ε: 0.476 steps:  38 state: [63] time: 125:12 score: 25 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  744 ε: 0.475 steps:  20 state: [46] time: 125:21 score: 25 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  745 ε: 0.475 steps:  52 state: [63] time: 125:35 score: 26 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  746 ε: 0.474 steps:  36 state: [63] time: 125:47 score: 27 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  747 ε: 0.474 steps:  25 state: [54] time: 125:56 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  748 ε: 0.473 steps:  30 state: [63] time: 126:07 score: 28 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  749 ε: 0.473 steps:  19 state: [29] time: 126:16 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  750 ε: 0.472 steps:  11 state: [19] time: 126:23 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  751 ε: 0.472 steps:  26 state: [63] time: 126:33 score: 29 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  752 ε: 0.471 steps:  18 state: [29] time: 126:41 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  753 ε: 0.471 steps:   9 state: [19] time: 126:48 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  754 ε: 0.470 steps:  11 state: [19] time: 126:55 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  755 ε: 0.470 steps:   5 state: [19] time: 127:02 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  756 ε: 0.469 steps:  24 state: [46] time: 127:11 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  757 ε: 0.469 steps:  19 state: [63] time: 127:20 score: 29 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  758 ε: 0.468 steps:  18 state: [63] time: 127:29 score: 30 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  759 ε: 0.468 steps:  22 state: [46] time: 127:38 score: 30 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  760 ε: 0.467 steps:   7 state: [19] time: 127:44 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  761 ε: 0.467 steps:  25 state: [63] time: 127:54 score: 30 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  762 ε: 0.467 steps:  17 state: [35] time: 128:02 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  763 ε: 0.466 steps:  13 state: [29] time: 128:10 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  764 ε: 0.466 steps:  13 state: [19] time: 128:18 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  765 ε: 0.465 steps:  19 state: [46] time: 128:26 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  766 ε: 0.465 steps:  39 state: [63] time: 128:38 score: 28 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  767 ε: 0.464 steps:   7 state: [19] time: 128:45 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  768 ε: 0.464 steps:  25 state: [19] time: 128:54 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  769 ε: 0.463 steps:  34 state: [63] time: 129:05 score: 27 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  770 ε: 0.463 steps:  22 state: [63] time: 129:14 score: 27 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  771 ε: 0.462 steps:  36 state: [63] time: 129:26 score: 28 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  772 ε: 0.462 steps:  21 state: [46] time: 129:35 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  773 ε: 0.461 steps:  21 state: [35] time: 129:44 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  774 ε: 0.461 steps:  21 state: [46] time: 129:53 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  775 ε: 0.461 steps:  22 state: [41] time: 130:02 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  776 ε: 0.460 steps:  22 state: [19] time: 130:11 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  777 ε: 0.460 steps:  16 state: [29] time: 130:19 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  778 ε: 0.459 steps:  13 state: [19] time: 130:27 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  779 ε: 0.459 steps:  15 state: [19] time: 130:35 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  780 ε: 0.458 steps:   9 state: [19] time: 130:42 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  781 ε: 0.458 steps:   9 state: [35] time: 130:49 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  782 ε: 0.457 steps:  16 state: [35] time: 130:57 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  783 ε: 0.457 steps:  17 state: [42] time: 131:05 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  784 ε: 0.456 steps:  11 state: [35] time: 131:13 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  785 ε: 0.456 steps:  29 state: [35] time: 131:23 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  786 ε: 0.455 steps:   9 state: [35] time: 131:31 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  787 ε: 0.455 steps:  30 state: [46] time: 131:41 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  788 ε: 0.455 steps:   5 state: [19] time: 131:47 score: 25 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  789 ε: 0.454 steps:  18 state: [35] time: 131:56 score: 25 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  790 ε: 0.454 steps:  19 state: [19] time: 132:05 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  791 ε: 0.453 steps:  22 state: [19] time: 132:14 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  792 ε: 0.453 steps:  18 state: [35] time: 132:22 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  793 ε: 0.452 steps:  34 state: [29] time: 132:33 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  794 ε: 0.452 steps:  11 state: [19] time: 132:40 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  795 ε: 0.451 steps:  45 state: [59] time: 132:53 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  796 ε: 0.451 steps:   9 state: [42] time: 133:00 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  797 ε: 0.450 steps:  12 state: [42] time: 133:07 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  798 ε: 0.450 steps:  40 state: [19] time: 133:20 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  799 ε: 0.450 steps:  35 state: [35] time: 133:31 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  800 ε: 0.449 steps:  16 state: [46] time: 133:39 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  801 ε: 0.449 steps:  13 state: [29] time: 133:47 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  802 ε: 0.448 steps:  27 state: [35] time: 133:57 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  803 ε: 0.448 steps:  27 state: [41] time: 134:07 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  804 ε: 0.447 steps:  28 state: [41] time: 134:17 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  805 ε: 0.447 steps:  15 state: [35] time: 134:25 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  806 ε: 0.446 steps:  19 state: [19] time: 134:33 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  807 ε: 0.446 steps:   9 state: [42] time: 134:40 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  808 ε: 0.446 steps:  19 state: [35] time: 134:50 score: 22 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  809 ε: 0.445 steps:  19 state: [42] time: 134:58 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  810 ε: 0.445 steps:  17 state: [42] time: 135:07 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  811 ε: 0.444 steps:   9 state: [41] time: 135:15 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  812 ε: 0.444 steps:   9 state: [41] time: 135:22 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  813 ε: 0.443 steps:  46 state: [41] time: 135:35 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  814 ε: 0.443 steps:  43 state: [19] time: 135:48 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  815 ε: 0.442 steps:  24 state: [41] time: 135:58 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  816 ε: 0.442 steps:  27 state: [42] time: 136:08 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  817 ε: 0.442 steps:  35 state: [19] time: 136:19 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  818 ε: 0.441 steps:  18 state: [35] time: 136:28 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  819 ε: 0.441 steps:  17 state: [41] time: 136:37 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  820 ε: 0.440 steps:   9 state: [41] time: 136:44 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  821 ε: 0.440 steps:  11 state: [29] time: 136:51 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  822 ε: 0.439 steps:  46 state: [29] time: 137:04 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  823 ε: 0.439 steps:  31 state: [29] time: 137:15 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  824 ε: 0.438 steps:  21 state: [63] time: 137:24 score: 21 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  825 ε: 0.438 steps:  39 state: [29] time: 137:36 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  826 ε: 0.438 steps:  47 state: [29] time: 137:50 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  827 ε: 0.437 steps:  23 state: [63] time: 137:59 score: 20 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  828 ε: 0.437 steps:  16 state: [19] time: 138:07 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  829 ε: 0.436 steps:  32 state: [52] time: 138:18 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  830 ε: 0.436 steps:   9 state: [35] time: 138:25 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  831 ε: 0.435 steps:  21 state: [29] time: 138:34 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  832 ε: 0.435 steps:  64 state: [35] time: 138:51 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  833 ε: 0.435 steps:  11 state: [29] time: 138:58 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  834 ε: 0.434 steps:  17 state: [29] time: 139:07 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  835 ε: 0.434 steps:  87 state: [29] time: 139:27 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  836 ε: 0.433 steps:  26 state: [29] time: 139:37 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  837 ε: 0.433 steps:   9 state: [19] time: 139:45 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  838 ε: 0.432 steps:  15 state: [19] time: 139:53 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  839 ε: 0.432 steps:  37 state: [63] time: 140:05 score: 15 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  840 ε: 0.432 steps:   9 state: [19] time: 140:12 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  841 ε: 0.431 steps:  37 state: [63] time: 140:25 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  842 ε: 0.431 steps:  22 state: [29] time: 140:34 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  843 ε: 0.430 steps:  44 state: [63] time: 140:47 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  844 ε: 0.430 steps:  24 state: [46] time: 140:57 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  845 ε: 0.429 steps:  15 state: [19] time: 141:05 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  846 ε: 0.429 steps: 100 state: [22] time: 141:28 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  847 ε: 0.429 steps:  20 state: [29] time: 141:37 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  848 ε: 0.428 steps:   7 state: [19] time: 141:44 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  849 ε: 0.428 steps:  25 state: [46] time: 141:53 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  850 ε: 0.427 steps:  57 state: [63] time: 142:09 score: 14 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  851 ε: 0.427 steps:  10 state: [29] time: 142:17 score: 13 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  852 ε: 0.426 steps:  36 state: [63] time: 142:29 score: 14 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  853 ε: 0.426 steps:  20 state: [63] time: 142:38 score: 15 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  854 ε: 0.426 steps:  29 state: [63] time: 142:49 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  855 ε: 0.425 steps:  21 state: [63] time: 142:58 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  856 ε: 0.425 steps:  25 state: [63] time: 143:09 score: 18 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  857 ε: 0.424 steps:  30 state: [54] time: 143:19 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  858 ε: 0.424 steps:  18 state: [63] time: 143:28 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  859 ε: 0.423 steps:  21 state: [29] time: 143:38 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  860 ε: 0.423 steps:  38 state: [63] time: 143:50 score: 18 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  861 ε: 0.423 steps:  10 state: [29] time: 143:58 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  862 ε: 0.422 steps:  42 state: [63] time: 144:11 score: 18 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  863 ε: 0.422 steps:  48 state: [54] time: 144:25 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  864 ε: 0.421 steps:  17 state: [29] time: 144:33 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  865 ε: 0.421 steps:  32 state: [29] time: 144:44 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  866 ε: 0.420 steps:   7 state: [19] time: 144:52 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  867 ε: 0.420 steps:  19 state: [19] time: 145:00 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  868 ε: 0.420 steps:   9 state: [19] time: 145:08 score: 17 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  869 ε: 0.419 steps:  15 state: [29] time: 145:16 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  870 ε: 0.419 steps:  18 state: [29] time: 145:24 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  871 ε: 0.418 steps:  14 state: [29] time: 145:33 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  872 ε: 0.418 steps:  18 state: [29] time: 145:42 score: 14 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  873 ε: 0.418 steps:  20 state: [63] time: 145:51 score: 15 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  874 ε: 0.417 steps:  14 state: [29] time: 145:59 score: 15 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  875 ε: 0.417 steps:  19 state: [63] time: 146:08 score: 16 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  876 ε: 0.416 steps:  16 state: [35] time: 146:17 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  877 ε: 0.416 steps:  25 state: [54] time: 146:27 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  878 ε: 0.415 steps:   5 state: [19] time: 146:33 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  879 ε: 0.415 steps:   5 state: [19] time: 146:40 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  880 ε: 0.415 steps:  20 state: [54] time: 146:49 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  881 ε: 0.414 steps:  14 state: [29] time: 146:57 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  882 ε: 0.414 steps:  21 state: [19] time: 147:07 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  883 ε: 0.413 steps:  45 state: [54] time: 147:20 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  884 ε: 0.413 steps:   5 state: [19] time: 147:27 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  885 ε: 0.413 steps:  46 state: [46] time: 147:40 score: 16 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  886 ε: 0.412 steps:  32 state: [63] time: 147:51 score: 17 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  887 ε: 0.412 steps:  32 state: [63] time: 148:02 score: 18 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  888 ε: 0.411 steps: 100 state: [47] time: 148:26 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  889 ε: 0.411 steps:   7 state: [19] time: 148:32 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  890 ε: 0.410 steps:   6 state: [41] time: 148:39 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  891 ε: 0.410 steps:   7 state: [41] time: 148:46 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  892 ε: 0.410 steps:  19 state: [46] time: 148:55 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  893 ε: 0.409 steps:   9 state: [19] time: 149:02 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  894 ε: 0.409 steps:  33 state: [54] time: 149:13 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  895 ε: 0.408 steps:  44 state: [46] time: 149:27 score: 18 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  896 ε: 0.408 steps:  42 state: [63] time: 149:39 score: 19 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  897 ε: 0.408 steps: 100 state: [22] time: 150:02 score: 19 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  898 ε: 0.407 steps:  51 state: [63] time: 150:17 score: 20 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  899 ε: 0.407 steps:  14 state: [19] time: 150:25 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  900 ε: 0.406 steps:  34 state: [29] time: 150:36 score: 20 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  901 ε: 0.406 steps:  26 state: [63] time: 150:46 score: 21 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  902 ε: 0.406 steps:  16 state: [29] time: 150:55 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  903 ε: 0.405 steps:  19 state: [46] time: 151:04 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  904 ε: 0.405 steps:  33 state: [46] time: 151:15 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  905 ε: 0.404 steps:   9 state: [19] time: 151:23 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  906 ε: 0.404 steps:  14 state: [29] time: 151:31 score: 21 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  907 ε: 0.404 steps:  30 state: [63] time: 151:42 score: 22 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  908 ε: 0.403 steps:  35 state: [63] time: 151:54 score: 23 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  909 ε: 0.403 steps:  25 state: [54] time: 152:04 score: 23 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  910 ε: 0.402 steps:  32 state: [63] time: 152:15 score: 24 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  911 ε: 0.402 steps:  33 state: [52] time: 152:27 score: 24 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  912 ε: 0.402 steps:  35 state: [63] time: 152:38 score: 25 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  913 ε: 0.401 steps:  17 state: [63] time: 152:47 score: 26 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  914 ε: 0.401 steps:   9 state: [35] time: 152:54 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  915 ε: 0.400 steps:  15 state: [19] time: 153:02 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  916 ε: 0.400 steps:  29 state: [63] time: 153:13 score: 27 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  917 ε: 0.400 steps:  18 state: [63] time: 153:22 score: 28 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  918 ε: 0.399 steps:  34 state: [63] time: 153:34 score: 29 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  919 ε: 0.399 steps:  46 state: [29] time: 153:47 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  920 ε: 0.398 steps:  50 state: [63] time: 154:02 score: 30 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  921 ε: 0.398 steps:  17 state: [19] time: 154:10 score: 30 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  922 ε: 0.398 steps:  22 state: [54] time: 154:20 score: 30 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  923 ε: 0.397 steps:   7 state: [19] time: 154:27 score: 30 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  924 ε: 0.397 steps:  21 state: [54] time: 154:36 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  925 ε: 0.396 steps:  17 state: [35] time: 154:45 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  926 ε: 0.396 steps:  32 state: [54] time: 154:56 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  927 ε: 0.396 steps:  34 state: [29] time: 155:08 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  928 ε: 0.395 steps:  17 state: [46] time: 155:17 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  929 ε: 0.395 steps:  40 state: [42] time: 155:30 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  930 ε: 0.394 steps:  19 state: [42] time: 155:39 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  931 ε: 0.394 steps:  58 state: [41] time: 155:55 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  932 ε: 0.394 steps: 100 state: [25] time: 156:17 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  933 ε: 0.393 steps:  36 state: [41] time: 156:29 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  934 ε: 0.393 steps:  32 state: [41] time: 156:40 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  935 ε: 0.392 steps:  85 state: [35] time: 157:00 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  936 ε: 0.392 steps:  29 state: [41] time: 157:10 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  937 ε: 0.392 steps:  88 state: [63] time: 157:31 score: 29 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  938 ε: 0.391 steps: 100 state: [33] time: 157:54 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  939 ε: 0.391 steps: 100 state: [28] time: 158:16 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  940 ε: 0.390 steps:  24 state: [41] time: 158:26 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  941 ε: 0.390 steps:  31 state: [63] time: 158:37 score: 28 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  942 ε: 0.390 steps:  19 state: [63] time: 158:46 score: 29 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  943 ε: 0.389 steps: 100 state: [32] time: 159:08 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  944 ε: 0.389 steps:  16 state: [63] time: 159:17 score: 29 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  16 steps: 16\n",
            "keep   > min_steps:  16 keep_cnt: 3\n",
            "episode:  945 ε: 0.388 steps:  27 state: [49] time: 159:27 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  946 ε: 0.388 steps:  51 state: [41] time: 159:41 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  947 ε: 0.388 steps:  12 state: [41] time: 159:49 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  948 ε: 0.387 steps:  55 state: [41] time: 160:05 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  949 ε: 0.387 steps:  55 state: [41] time: 160:19 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  950 ε: 0.387 steps:  20 state: [29] time: 160:29 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  951 ε: 0.386 steps:  27 state: [29] time: 160:39 score: 28 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  952 ε: 0.386 steps:  20 state: [63] time: 160:48 score: 28 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  953 ε: 0.385 steps:  31 state: [54] time: 160:58 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  954 ε: 0.385 steps:  37 state: [63] time: 161:10 score: 27 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  955 ε: 0.385 steps:  21 state: [35] time: 161:19 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  956 ε: 0.384 steps:  20 state: [63] time: 161:28 score: 26 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  957 ε: 0.384 steps:  25 state: [54] time: 161:38 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  958 ε: 0.383 steps:  21 state: [63] time: 161:47 score: 26 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  959 ε: 0.383 steps:  20 state: [63] time: 161:56 score: 27 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  960 ε: 0.383 steps:   5 state: [19] time: 162:03 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  961 ε: 0.382 steps:  14 state: [29] time: 162:11 score: 26 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  962 ε: 0.382 steps:  17 state: [19] time: 162:19 score: 25 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  963 ε: 0.382 steps:  36 state: [63] time: 162:31 score: 26 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  964 ε: 0.381 steps:  21 state: [63] time: 162:40 score: 27 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  965 ε: 0.381 steps:  15 state: [49] time: 162:48 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  966 ε: 0.380 steps:  20 state: [46] time: 162:57 score: 27 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  967 ε: 0.380 steps:  34 state: [63] time: 163:08 score: 28 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  968 ε: 0.380 steps:  28 state: [63] time: 163:19 score: 29 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  969 ε: 0.379 steps:  24 state: [54] time: 163:28 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  970 ε: 0.379 steps:   5 state: [19] time: 163:34 score: 29 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  971 ε: 0.379 steps:  24 state: [63] time: 163:44 score: 30 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "episode:  972 ε: 0.378 steps:  21 state: [46] time: 163:53 score: 30 memory:5000 reward: \u001b[91m+0.0\u001b[0m\n",
            "episode:  973 ε: 0.378 steps:  16 state: [63] time: 164:02 score: 30 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  16 steps: 16\n",
            "keep   > min_steps:  16 keep_cnt: 4\n",
            "episode:  974 ε: 0.377 steps:  16 state: [63] time: 164:10 score: 31 memory:5000 reward: \u001b[92m+1.0\u001b[0m\n",
            "------------------->  pre min_steps:  16 steps: 16\n",
            "keep   > min_steps:  16 keep_cnt: 5\n",
            "974 early stopping!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_solver.display_report()"
      ],
      "metadata": {
        "id": "-Zb2LGdEam4y",
        "outputId": "1f64aa94-0c9d-4bea-a555-ce3e0e085aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-table 시각화\n",
        "def print_str_direct(q_value, map_size):\n",
        "    cnt = 0\n",
        "    while cnt < len(q_value):\n",
        "        txt = ''\n",
        "        for _ in range(map_size):\n",
        "            # q-value가 실수인 경우 보완\n",
        "            q = ''.join([str(int(round(e, 0))) for e in q_value[cnt]])\n",
        "            if q == '1000':\n",
        "                txt += STR_ACTION_LEFT\n",
        "            elif q == '0100':\n",
        "                txt += STR_ACTION_DOWN\n",
        "            elif q == '0010':\n",
        "                txt += STR_ACTION_RIGHT\n",
        "            elif q == '0001':\n",
        "                txt += STR_ACTION_UP\n",
        "            else:\n",
        "                txt += ' '\n",
        "            txt += ' | '\n",
        "            cnt += 1\n",
        "        print(txt)\n",
        "\n",
        "def get_optimal_path(optimal_q_value, map_size):\n",
        "    list_optimal_step = []\n",
        "    optimal_step = 0\n",
        "    optimal_path = []\n",
        "\n",
        "    #q-value중 max값을 1로 변경\n",
        "    qvalue_table = np.zeros([optimal_q_value.shape[0], optimal_q_value.shape[1]])\n",
        "\n",
        "    for state, q_value in enumerate(optimal_q_value):\n",
        "        q_max = np.amax(q_value)  # q_value array의 최댓값 반환\n",
        "        indices = np.nonzero(q_value == q_max)[0]\n",
        "        qvalue_table[state, indices[0]] = 1\n",
        "        state += 1\n",
        "\n",
        "    #print(qvalue_table)\n",
        "\n",
        "    for state, q_value in enumerate(qvalue_table):\n",
        "        index = q_value.argmax()\n",
        "\n",
        "        if optimal_step == state : # 최단 경로 위에 있는 state에 대해 최적경로step을 지정한다.\n",
        "            list_optimal_step.append(optimal_step)\n",
        "\n",
        "            if optimal_step == GOAL_STATE:\n",
        "                q_value = [0,0,0,0]\n",
        "            else:   \n",
        "                if index == IDX_ACTION_UP and state not in WALL_UPSIDE:\n",
        "                    optimal_step -= map_size\n",
        "                elif index == IDX_ACTION_DOWN and state not in WALL_DOWNSIDE:\n",
        "                    optimal_step += map_size\n",
        "                elif index == IDX_ACTION_RIGHT and state not in WALL_RIGHTSIDE:\n",
        "                    optimal_step += 1\n",
        "                elif index == IDX_ACTION_LEFT and state not in WALL_LEFTSIDE:\n",
        "                    optimal_step -= 1\n",
        "                else: \n",
        "                    pass\n",
        "        else:\n",
        "            q_value = [0,0,0,0]\n",
        "        \n",
        "        optimal_path.append(list(map(int,q_value))) #[1.0, 0.0, 0.0, 0.0] -> [1,0,0,0]\n",
        "\n",
        "    #print(list_optimal_step)\n",
        "    #print(optimal_path)\n",
        "\n",
        "    if optimal_step != GOAL_STATE:\n",
        "        print(\"Agent can't find optimal path.\")\n",
        "    return optimal_path"
      ],
      "metadata": {
        "id": "6ZNPyHsY0g0z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_size = dqn_solver.observation_space\n",
        "action_size = dqn_solver.action_space\n",
        "q_table = np.zeros([state_size, action_size])\n",
        "\n",
        "for state in range(state_size): \n",
        "    q_value = dqn_solver.model.predict(np.identity(state_size)[state:state + 1])\n",
        "    q_table[state] = q_value\n",
        "    \n",
        "optimal_path = get_optimal_path(q_table, 8)\n",
        "print_str_direct(optimal_path, 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF6yt5GEOkYc",
        "outputId": "dc47e33c-5e65-48aa-f780-b477690802a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D |   |   |   |   |   |   |   | \n",
            "R | R | R | R | R | R | R | D | \n",
            "  |   |   |   |   |   |   | D | \n",
            "  |   |   |   |   |   |   | D | \n",
            "  |   |   |   |   |   |   | D | \n",
            "  |   |   |   |   |   |   | D | \n",
            "  |   |   |   |   |   |   | D | \n",
            "  |   |   |   |   |   |   |   | \n"
          ]
        }
      ]
    }
  ]
}