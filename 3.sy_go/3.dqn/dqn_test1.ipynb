{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_test1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rVRhUSAMS9yv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque \n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from gym.envs.registration import register\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA=0.9 #dis = .99\n",
        "EPSILON=0.1\n",
        "REPLAY_BUFFER_SIZE = 2000\n",
        "learning_rate = 0.1 #ALPHA=0.4  # learning rate\n",
        "batch_size = 256 #32 # 훈련을 위해 채울 메모리 사이즈\n",
        "\n",
        "# Set Q-learning parameters\n",
        "num_episodes = 100\n",
        "\n",
        "#환경 변수\n",
        "IDX_ACTION_LEFT = 0\n",
        "IDX_ACTION_DOWN = 1\n",
        "IDX_ACTION_RIGHT = 2\n",
        "IDX_ACTION_UP = 3\n",
        "\n",
        "STR_ACTION_UP = 'U'\n",
        "STR_ACTION_DOWN = 'D'\n",
        "STR_ACTION_RIGHT = 'R'\n",
        "STR_ACTION_LEFT = 'L'\n",
        "\n",
        "GOAL_STATE = 15\n",
        "\n"
      ],
      "metadata": {
        "id": "Kk-XXrn6dqWf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = 'FrozenLake-v0'\n",
        "env_dict = gym.envs.registry.env_specs.copy()\n",
        " \n",
        "for env in env_dict:\n",
        "    if env_id in env:\n",
        "        print('Remove {} from registry'.format(env))\n",
        "        del gym.envs.registry.env_specs[env]   \n",
        "\n",
        "register(\n",
        "    id=env_id,\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '4x4',\n",
        "            'is_slippery': False}\n",
        ")\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# Input and output size based on the Env\n",
        "input_size = env.observation_space.n\n",
        "output_size = env.action_space.n\n",
        "\n",
        "\n",
        "print('취할 수 있는 상태 수: {}'.format(env.observation_space.n))\n",
        "print('취할 수 있는 행동 수: {}'.format(env.action_space.n))\n",
        "print('Q table size : {}x{}'.format(env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# weight\n",
        "#W = tf.Variable(tf.random.uniform([input_size, output_size], 0, 0.01), dtype=tf.float32)\n",
        "\n",
        "#optimizer = tf.optimizers.SGD(learning_rate=learning_rate)\n",
        "optimizer = Adam(learning_rate=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOUA-dRHICrf",
        "outputId": "120859f3-ac4f-44bc-816f-a14daa8a3f6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remove FrozenLake-v0 from registry\n",
            "취할 수 있는 상태 수: 16\n",
            "취할 수 있는 행동 수: 4\n",
            "Q table size : 16x4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, optimizer):  \n",
        "        self._state_size = env.observation_space.n\n",
        "        self._action_size = env.action_space.n\n",
        "        self._optimizer = optimizer\n",
        "        self.expirience_replay = deque(maxlen = REPLAY_BUFFER_SIZE) # 에이전트가 환경에 반응한 경험을 저장\n",
        "        \n",
        "        self.gamma = GAMMA # 할인율 초기화\n",
        "        self.epsilon = EPSILON # 탐험 비율 초기화\n",
        "\n",
        "        self.q_network = self.build_compile() # 큐-네트워크 구성\n",
        "        self.target_network = self.build_compile() #  타깃 큐-네트워크 구성\n",
        "        self.target_model() # 가중치를 적용\n",
        "\n",
        "    def store(self, state, action, reward, next_state, terminated):\n",
        "        self.expirience_replay.append((state, action, reward, next_state, terminated))\n",
        "\n",
        "    # Embedding\n",
        "    # _state_size : 입력에 대한 차원(총 입력 개수)으로 500\n",
        "    # 출력에 대한 차원(결과로 나오는 임베딩 벡터의 크기)으로 10\n",
        "    # input_length : 입력 시퀀스의 길이\n",
        "    def build_compile(self):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(self._state_size, 10, input_length=1))\n",
        "        model.add(Reshape((10,)))\n",
        "        model.add(Dense(50, activation='relu'))\n",
        "        model.add(Dense(50, activation='relu'))\n",
        "        model.add(Dense(50, activation='relu'))\n",
        "        model.add(Dense(self._action_size, activation='linear'))  # 마지막 layer : action 수만큼 hidden units 설정\n",
        "        model.compile(loss='mse', optimizer=self._optimizer)\n",
        "        return model\n",
        "\n",
        "    def target_model(self):\n",
        "        self.target_network.set_weights(self.q_network.get_weights()) #타겟 네트워크에 저장\n",
        "\n",
        "    def get_target_weights(self):\n",
        "        return self.target_network.get_weights()\n",
        "\n",
        "    def act(self, state, epsilon): \n",
        "        #if np.random.rand() <= self.epsilon:\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return env.action_space.sample()\n",
        "        q_values = self.q_network.predict(state) \n",
        "        return np.argmax(q_values[0]) \n",
        "\n",
        "    def retrain(self, batch_size): #  큐-네트워크 훈련\n",
        "        minibatch = random.sample(self.expirience_replay, batch_size) # 샘플링: 리플레이 메모리에서 랜덤한 데이터 선택\n",
        "\n",
        "        for state, action, reward, next_state, terminated in minibatch:\n",
        "            target = self.q_network.predict(state)\n",
        "            if terminated:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                t = self.target_network.predict(next_state)\n",
        "                target[0][action] = reward + self.gamma * np.amax(t)\n",
        "\n",
        "            self.q_network.fit(state, target, epochs=1, verbose=0) # 큐-네트워크 훈련"
      ],
      "metadata": {
        "id": "p-_LAgcfd_yW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "agent = Agent(env, optimizer)\n",
        "#agent.q_network.summary()\n",
        "\n",
        "rList = []\n",
        "min_act = env.observation_space.n * env.action_space.n\n",
        "optimal_W = []\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    b_success = False   # goal에 도착한경우\n",
        "    action_cnt = 0      # action 횟수 설정\n",
        "\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, 1])\n",
        "    \n",
        "    e = 1. / ((i / 50) + 10)\n",
        "    \n",
        "    done = False\n",
        "    rAll = 0    \n",
        "    reward = 0 #보상 변수 초기화    \n",
        "    action_cnt = 0      # action 횟수 설정\n",
        "    \n",
        "    while not done:\n",
        "        action = agent.act(state, e) \n",
        "        state_next, reward, done, info = env.step(action)  \n",
        "        state_next = np.reshape(state_next, [1, 1]) #Q = R + Q\n",
        "        agent.store(state, action, reward, state_next, done)\n",
        "\n",
        "        rAll += reward\n",
        "        #if timestep == 0 : print('ep[',e,']', 'state:', state, ' action:', action, ' reward:', reward, ' state_next:', state_next, 'total r:', rAll) \n",
        "        \n",
        "        if done:\n",
        "            if state_next == [[15]]:\n",
        "                print('Find --> ep[',i,']', 'state:', state, ' action:', action, ' reward:', reward, ' next_s:', state_next, 'total r:', rAll) \n",
        "            else:\n",
        "                print('Hole --> ep[',i,']', 'state:', state, ' action:', action, ' reward:', reward, ' next_s:', state_next, 'total r:', rAll)           \n",
        "            break\n",
        "\n",
        "        state = state_next\n",
        "        action_cnt +=1\n",
        "\n",
        "        if state_next == GOAL_STATE:\n",
        "            b_success = True\n",
        "\n",
        "        if len(agent.expirience_replay) > batch_size: # 어느정도 경험이 쌓인후 다시 학습시작\n",
        "            agent.retrain(batch_size)\n",
        "\n",
        "    rList.append(rAll)\n",
        "\n",
        "    if i % 10 ==1: # 10번마다 targetDQN으로 복사\n",
        "        agent.target_model() #q-tartget network 의 q 값을 재설정\n",
        "\n",
        "    # 최단거리로 Goal간경우 q_value를 optimal value로 설정\n",
        "    if b_success and action_cnt < min_act:            \n",
        "        min_act = action_cnt\n",
        "        optimal_W = agent.get_target_weights()\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(\"**********************************\")\n",
        "        print(\"Episode: {}\".format(i + 1))\n",
        "        env.render() \n",
        "        print(\"**********************************\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-1pMG9jlSCA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "1e35087d-425c-48a3-a41a-401648ed6ed1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hole --> ep[ 0 ] state: [[4]]  action: 2  reward: 0.0  next_s: [[5]] total r: 0.0\n",
            "Hole --> ep[ 1 ] state: [[8]]  action: 1  reward: 0.0  next_s: [[12]] total r: 0.0\n",
            "Hole --> ep[ 2 ] state: [[8]]  action: 1  reward: 0.0  next_s: [[12]] total r: 0.0\n",
            "Hole --> ep[ 3 ] state: [[8]]  action: 1  reward: 0.0  next_s: [[12]] total r: 0.0\n",
            "Hole --> ep[ 4 ] state: [[8]]  action: 1  reward: 0.0  next_s: [[12]] total r: 0.0\n",
            "Hole --> ep[ 5 ] state: [[8]]  action: 1  reward: 0.0  next_s: [[12]] total r: 0.0\n",
            "Hole --> ep[ 6 ] state: [[1]]  action: 1  reward: 0.0  next_s: [[5]] total r: 0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ac43bbefc694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpirience_replay\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 어느정도 경험이 쌓인후 다시 학습시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mrList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrAll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-2bb64b846789>\u001b[0m in \u001b[0;36mretrain\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1976\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1978\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1979\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 755\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3315\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3316\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3318\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{(time.time() - start_time)} seconds')\n",
        "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
        "plt.bar(range(len(rList)), rList, color='b', alpha=0.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jJwHoENZKjfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-table 시각화\n",
        "def print_str_direct(q_value):\n",
        "    cnt = 0\n",
        "    while cnt < len(q_value):\n",
        "        txt = ''\n",
        "        for _ in range(4):\n",
        "            # q-value가 실수인 경우 보완\n",
        "            q = ''.join([str(int(round(e, 0))) for e in q_value[cnt]])\n",
        "            if q == '1000':\n",
        "                txt += STR_ACTION_LEFT\n",
        "            elif q == '0100':\n",
        "                txt += STR_ACTION_DOWN\n",
        "            elif q == '0010':\n",
        "                txt += STR_ACTION_RIGHT\n",
        "            elif q == '0001':\n",
        "                txt += STR_ACTION_UP\n",
        "            else:\n",
        "                txt += ' '\n",
        "            txt += ' | '\n",
        "            cnt += 1\n",
        "        print(txt)    \n",
        "        \n",
        "def one_hot(x):\n",
        "    return np.identity(16)[x:x+1].astype(np.float32)\n"
      ],
      "metadata": {
        "id": "h1xOYbbMZ2f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimal_path(optimal_q_value):\n",
        "    list_optimal_step = []\n",
        "    optimal_step = 0\n",
        "    optimal_path = []\n",
        "\n",
        "    #q-value중 max값을 1로 변경\n",
        "    qvalue_table = np.zeros([optimal_q_value.shape[0], optimal_q_value.shape[1]])\n",
        "\n",
        "    for state, q_value in enumerate(optimal_q_value):\n",
        "        q_max = np.amax(q_value)  # q_value array의 최댓값 반환\n",
        "        indices = np.nonzero(q_value == q_max)[0]\n",
        "        qvalue_table[state, indices[0]] = 1\n",
        "        state += 1\n",
        "\n",
        "    #print(qvalue_table)\n",
        "\n",
        "    for state, q_value in enumerate(qvalue_table):\n",
        "        index = q_value.argmax()\n",
        "\n",
        "        if optimal_step == state : # 최단 경로 위에 있는 state에 대해 최적경로step을 지정한다.\n",
        "            list_optimal_step.append(optimal_step)\n",
        "\n",
        "            if optimal_step == GOAL_STATE:\n",
        "                q_value = [0,0,0,0]\n",
        "            else:   \n",
        "                if index == IDX_ACTION_UP :\n",
        "                    optimal_step -= 4\n",
        "                elif index == IDX_ACTION_DOWN :\n",
        "                    optimal_step += 4\n",
        "                elif index == IDX_ACTION_RIGHT :\n",
        "                    optimal_step += 1\n",
        "                elif index == IDX_ACTION_LEFT :\n",
        "                    optimal_step -= 1\n",
        "                else: \n",
        "                    pass\n",
        "        else:\n",
        "            q_value = [0,0,0,0]\n",
        "        \n",
        "        optimal_path.append(list(map(int,q_value)))\n",
        "\n",
        "    #print(list_optimal_step)\n",
        "    #print(optimal_path)\n",
        "\n",
        "    if optimal_step != GOAL_STATE:\n",
        "        print(\"Agent can't find optimal path.\")\n",
        "    return optimal_path"
      ],
      "metadata": {
        "id": "xok4dZ7XaCVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{(time.time() - start_time)} seconds')\n",
        "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
        "plt.bar(range(len(rList)), rList, color='b', alpha=0.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RvIhnpS1aHvg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}