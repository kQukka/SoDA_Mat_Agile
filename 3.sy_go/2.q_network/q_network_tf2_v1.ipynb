{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q_network_tf2_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Network\n",
        "\n",
        "Leture 6. Q-Network\n",
        "- [강의 슬라이드](http://hunkim.github.io/ml/RL/rl06.pdf)\n",
        "- [강의 동영상](https://www.youtube.com/watch?v=w9GwqPx7LW8)\n"
      ],
      "metadata": {
        "id": "YcEH3OhAS0b_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla Q-Learning vs Q-Network\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FclZLp2%2FbtrATdqwjsW%2FLsYcA3Q89jwZlRX2sU0aVk%2Fimg.png\" width=\"400\" height=\"300\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "BpDaFFtbQRbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-Learning\n",
        "---\n",
        "- 큐-러닝 : 에이전트가 주어진 상태에서 행동을 취했을 경우 받을 수 있는 보상의 기댓값을 예측하는 큐-함수(Q-function)를 사용하여 최적화된 정책을 학습하는 강화 학습 기법이다.\n",
        "- 큐러닝의 한계 : state-action에 따른 값들을 모두 Table 형태(Q-Table)로 구성되므로 에이전트가 취할 수 있는 상태 개수가 많은 경우 큐-테이블 구축에 한계가 발생한다."
      ],
      "metadata": {
        "id": "Cp8tn3k2OePc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Q-Network \n",
        "---\n",
        "\n",
        "- 신경망을 활용한 Q-learning : 현재 상태를 입력으로 주었을때 취할수 있는 모든 행동에 대한 Q-value를 반환한다.\n",
        " <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbzdecs%2FbtrAQgIfEpI%2FK4batbxm9BBcugKt6G3Mk1%2Fimg.png\" width=\"400\" height=\"50\"/>\n",
        "\n",
        " - 결과값은 Linear Regressing(선형회귀)의 결과와 비슷한 Ws로 설정\n",
        " - Cost Function(비용함수) : Ws와 Label값인 y의 차를 제곱한 값들을 더한 값으로 설정된다.\n",
        "\n",
        "<br>\n",
        "\n",
        "- 비용에 대한 최소화를 공식화\n",
        "\n",
        " <img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbjaUIW%2FbtrATS1ksOK%2FkPkiiJT659kvhiPwykckG1%2Fimg.png\" width=\"400\" height=\"150\"/>\n",
        "\n",
        " - 목표값 : Q*(Optimal Q) = reward + dis * np.max(q_score_next) \n",
        " - 예측값 : Q_hat(Q prediction) = Ws = tf.matmul(one_hot(state), W)\n"
      ],
      "metadata": {
        "id": "PEeHNEPXipdR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcoZl_KgPx8a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(x):\n",
        "    return np.identity(16)[x:x+1].astype(np.float32)"
      ],
      "metadata": {
        "id": "WUH_8s7jP01t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "# Set Q-learning parameters\n",
        "num_episodes = 2000\n",
        "learning_rate = 0.1\n",
        "dis = .99\n",
        "\n",
        "# Input and output size based on the Env\n",
        "input_size = env.observation_space.n\n",
        "output_size = env.action_space.n\n",
        "\n",
        "# weight\n",
        "W = tf.Variable(tf.random.uniform([input_size, output_size], 0, 0.01), dtype=tf.float32)\n",
        "# optimizer \n",
        "optimizer = tf.optimizers.SGD(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "R3G51aUuP2qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# rewards per episode\n",
        "rList = []\n",
        "for i in range(num_episodes):\n",
        "    # Reset environment and get first new observation\n",
        "    state = env.reset()\n",
        "    rAll = 0\n",
        "    done = False\n",
        "    local_loss = []\n",
        "\n",
        "    e = 1. / ((i / 50) + 10)\n",
        "    # The Q-Table learning algorithm\n",
        "    while not done:\n",
        "        # Choose an action by greedly (with a chance of random action) from the Q-network\n",
        "        q_value = tf.matmul(one_hot(state), W)  \n",
        "        q_value = np.array(q_value.numpy())\n",
        "\n",
        "        if np.random.rand(1) < e:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_value)\n",
        "\n",
        "        #--------------------------------------------------------------------\n",
        "        # Set y label\n",
        "        #--------------------------------------------------------------------\n",
        "        # Get new state and reward from environment\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        if done:\n",
        "            # Update Q, and no q_value+1, since it's action termial state\n",
        "            q_value[0, action] = reward\n",
        "        else:\n",
        "            # Obtain the Q_s` values by feeding the new state through our network\n",
        "            q_score_next = tf.matmul(one_hot(state_next), W)  \n",
        "            # Update Q\n",
        "            q_value[0, action] = reward + dis * np.max(q_score_next) \n",
        "\n",
        "        #--------------------------------------------------------------------\n",
        "        # Train our network using target (Y) and predicted Q(Q_pred) values\n",
        "        # Q_pred : tf.matmul(one_hot(state), W))\n",
        "        #--------------------------------------------------------------------\n",
        "        loss = lambda: tf.reduce_sum(input_tensor=tf.square(q_value - tf.matmul(one_hot(state), W)))\n",
        "\n",
        "        # optimizer, loss가 작아지는 방향으로 W 업데이트\n",
        "        optimizer.minimize(loss, var_list=W)\n",
        "        rAll += reward\n",
        "        state = state_next\n",
        "    rList.append(rAll)"
      ],
      "metadata": {
        "id": "WN3ZHwKhP2nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{(time.time() - start_time)} seconds')\n",
        "print(\"Success rate: \" + str(sum(rList) / num_episodes))\n",
        "plt.bar(range(len(rList)), rList, color='b', alpha=0.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9s8c59T8P2kQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "81a5059c-863c-4996-9739-9636fdb77dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "220.44312620162964 seconds\n",
            "Success rate: 0.413\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Network 의 한계\n",
        "Q Network의 결과는 아래의 문제점으로 Q-Table 보다 성능이 좋지 않다.\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FYbJRV%2FbtrAYlueDcJ%2FhPi4stPGEGSZq71LESqYK1%2Fimg.png\" width=\"400\" height=\"300\"/>\n",
        "\n",
        "\n",
        "- Correlation \n",
        " - 연속된 sample들로 학습시 연관성이 커 학습이 되지 않는다.\n",
        " - 전체적인 특징을 파악하기 어렵다.\n",
        "- Non-stationary target \n",
        " - Target값과 예측값을 구하는데 같은 모델을 사용하여 네트워크가 업데이트되면 Target값이 변하게 된다.\n",
        " - label에 해당하는 Target값이 변하므로 학습이 안정적으로 이루어지지 않는다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JzwRFCSujUxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference\n",
        "- [Deep Q Network (DQN)](https://engineering-ladder.tistory.com/68?category=826971)"
      ],
      "metadata": {
        "id": "ZQeIcARvemyN"
      }
    }
  ]
}
