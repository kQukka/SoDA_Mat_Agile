{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"q_learning_lr_rev1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOxJ0xhVSwtTWLKM9u5HYH5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MctdEb5oJUbI"},"outputs":[],"source":["import gym\n","import numpy as np\n","from gym.envs.registration import register\n","import random as pr"]},{"cell_type":"code","source":["NUM_EPISODES = 2000\n","POINT_WALL = [[1, 1], [1, 3], [2, 3], [3, 0]]\n","POINT_GOAL = [[3, 3]]\n"],"metadata":{"id":"f8GbsBSyJV4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rargmax(vector):\n","    # np,amax = array의 최댓값 반환\n","    m = np.amax(vector)\n","    indices = np.nonzero(vector == m)[0]\n","    # 0이 아닌값 중에 m과 같은 값이 있으면\n","    # pr.choice(indices)\n","    # indices 중 random으로 choice\n","    return pr.choice(indices)\n","    "],"metadata":{"id":"xZHjAggJMgWw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_episodes(env, q_value, idx=0, greedy=False, noise=False, learning_rate=0, discount=1):\n","    done = False\n","    rAll = 0\n","    # 초기 state로 설정\n","    state = env.reset()\n","\n","    buf_q = []\n","    buf_act = []\n","    while not done:\n","        # e-greedy\n","        action = None\n","    \n","        # e-greedy\n","        if greedy:\n","            e = 1. / ((idx // 1000) + 1)\n","            if np.random.rand(1) < e:\n","                action = env.action_space.sample()\n","            else:\n","                action = rargmax(q_value[state, :])\n","        # noise\n","        elif noise:\n","            action = np.argmax(q_value[state, :]+np.random.randn(1, env.action_space.n)/(idx+1))\n","        else:\n","            action = rargmax(q_value[state, :])\n","    \n","        # done : learning 종료 (목적지 도착)\n","        new_state, reward, done, _ = env.step(action)\n","\n","        # if state != new_state:\n","        #     q_value[state, action] = reward + discount * np.max(q_value[new_state, :])\n","\n","        if learning_rate>0:\n","            q_value[state,action] = (1-learning_rate) * q_value[state,action] + learning_rate * (reward + discount*np.max(q_value[new_state, :]))\n","        else:\n","            q_value[state, action] = reward + discount * np.max(q_value[new_state, :])\n","\n","        buf_q.append(list(q_value[state]))\n","        if done:\n","            buf_q.append(list(q_value[new_state]))\n","        buf_act.append(action)\n","        rAll += reward\n","        state = new_state\n","    return buf_q, rAll, buf_act"],"metadata":{"id":"f_SQUhlNbYIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def run_q_learning(env, num_episodes, greedy=False, noise=False, learning_rate=0, discount=1):\n","    # Q-value table 생성\n","    q_value = np.zeros([env.observation_space.n, env.action_space.n])\n","\n","    # reword 저장\n","    log_reword = []\n","    log_q_by_step = []\n","    log_action = []\n","    log_q_map = []\n","    log_e = []\n","    for i in range(num_episodes):\n","        e = 1. / ((i // 1000) + 1)\n","        log_e.append(e)\n","\n","        buf_q = None\n","        rAll = None\n","        buf_act = None\n","        buf_q, rAll, buf_act = run_episodes(env, q_value, idx=i, greedy=greedy, noise=noise, learning_rate=learning_rate, discount=discount)\n","\n","        log_action.append(buf_act)\n","        log_q_by_step.append(buf_q)\n","        log_reword.append(rAll)\n","        log_q_map.append(q_value.copy())\n","    return q_value, log_reword, log_q_map, log_action, log_q_by_step\n"],"metadata":{"id":"vv7Kn-fJbaY0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stochastic (non-deterministic) 환경"],"metadata":{"id":"KFv_Yu9XsPcm"}},{"cell_type":"code","source":["# entry_point : gym.envs 환경 불러오기\n","register(\n","    id='LakeEnv-v1',\n","    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n","    kwargs={'map_name': '4x4', 'is_slippery': True}\n",")\n","env = gym.make('LakeEnv-v1')\n"],"metadata":{"id":"fCSQqTnpNBGC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q-learning 기본\n"],"metadata":{"id":"NCXWgSRqrrG5"}},{"cell_type":"code","source":["result_ = []\n","cnt = 100\n","for _ in range(cnt):\n","    q_value, log_reword, log_q_map, log_action, log_q_by_step  = run_q_learning(env, NUM_EPISODES)\n","    res = sum(log_reword) / NUM_EPISODES\n","    result_.append(res)\n","\n","print('Max success rate : ' + str(max(result_)))\n","print('Avg success rate : ' + str(sum(result_)/cnt))\n","print('\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4V9-tt6pyqH","executionInfo":{"status":"ok","timestamp":1651157586565,"user_tz":-540,"elapsed":69625,"user":{"displayName":"Jung Hyun Lee","userId":"01154420143394984910"}},"outputId":"38a4170e-63a4-43b5-cf6a-20f4cb66aecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max success rate : 0.0335\n","Avg success rate : 0.02069499999999999\n","\n","\n"]}]},{"cell_type":"markdown","source":["## Q-learning, Learning rate 적용"],"metadata":{"id":"pZET1BbbazyD"}},{"cell_type":"code","source":["cnt = 100\n","for num in range(1, 11):\n","    result_ = []\n","    lr = num/10\n","    for _ in range(cnt):\n","        q_value, log_reword, log_q_map, log_action, log_q_by_step  = run_q_learning(env, NUM_EPISODES, learning_rate=lr)\n","        res = sum(log_reword) / NUM_EPISODES\n","        result_.append(res)\n","\n","    print('Learning rate : ' + str(num/10))\n","    print('Max success rate : ' + str(max(result_)))\n","    print('Avg success rate : ' + str(sum(result_)/cnt))\n","    print('\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pPwdGUNNC9F","executionInfo":{"status":"ok","timestamp":1651162057987,"user_tz":-540,"elapsed":950967,"user":{"displayName":"Jung Hyun Lee","userId":"01154420143394984910"}},"outputId":"0fadad36-2bc8-477a-ac49-44efa6c15cbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate : 0.1\n","Max success rate : 0.4325\n","Avg success rate : 0.06584000000000005\n","\n","\n","Learning rate : 0.2\n","Max success rate : 0.1985\n","Avg success rate : 0.05355000000000001\n","\n","\n","Learning rate : 0.3\n","Max success rate : 0.263\n","Avg success rate : 0.06394500000000003\n","\n","\n","Learning rate : 0.4\n","Max success rate : 0.5625\n","Avg success rate : 0.06387\n","\n","\n","Learning rate : 0.5\n","Max success rate : 0.3095\n","Avg success rate : 0.051740000000000015\n","\n","\n","Learning rate : 0.6\n","Max success rate : 0.204\n","Avg success rate : 0.05086999999999999\n","\n","\n","Learning rate : 0.7\n","Max success rate : 0.352\n","Avg success rate : 0.05635499999999999\n","\n","\n","Learning rate : 0.8\n","Max success rate : 0.1905\n","Avg success rate : 0.05337499999999999\n","\n","\n","Learning rate : 0.9\n","Max success rate : 0.2535\n","Avg success rate : 0.0622\n","\n","\n","Learning rate : 1.0\n","Max success rate : 0.036\n","Avg success rate : 0.021775\n","\n","\n"]}]},{"cell_type":"markdown","source":["- 현재 state의 q_value를 고려하여 현재 state의 q_value를 업데이트\n","- reword와 다음 state의 q_value를 고려한 기본 q-learning에 비해 높은 성공률를 보여줌\n","- 학습 시도 중 최대 성공률이 상대적으로 매우 높은 경우가 있음\n","\n","- learning rate이 클수록 현재 state의 q_value보다 다음 state의 q_value를 더 많이 고려\n","- learning rate에 따라 다른 성공률를 보임\n","- 학습에 따라 learning rate를 선정할 필요가 있음\n"],"metadata":{"id":"ukWqa-das56Z"}},{"cell_type":"markdown","source":["## Q-learning, Learning rate + discounted reward 적용"],"metadata":{"id":"jhlAX4Opa0UD"}},{"cell_type":"code","source":["result_ = []\n","cnt = 100\n","for num in range(1, 11):\n","    result_ = []\n","    lr = num/10\n","    for _ in range(cnt):\n","        q_value, log_reword, log_q_map, log_action, log_q_by_step  = run_q_learning(env, NUM_EPISODES, learning_rate=lr, discount=0.9)\n","        res = sum(log_reword) / NUM_EPISODES\n","        result_.append(res)\n","\n","    print('Learning rate : ' + str(num/10))\n","    print('Max success rate : ' + str(max(result_)))\n","    print('Avg success rate : ' + str(sum(result_)/cnt))\n","    print('\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IZaczlB0NDae","executionInfo":{"status":"ok","timestamp":1651162980809,"user_tz":-540,"elapsed":922825,"user":{"displayName":"Jung Hyun Lee","userId":"01154420143394984910"}},"outputId":"1e3ee179-291d-4276-ae60-af7c1c53c677"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate : 0.1\n","Max success rate : 0.2555\n","Avg success rate : 0.05218\n","\n","\n","Learning rate : 0.2\n","Max success rate : 0.2035\n","Avg success rate : 0.05248500000000003\n","\n","\n","Learning rate : 0.3\n","Max success rate : 0.222\n","Avg success rate : 0.05266\n","\n","\n","Learning rate : 0.4\n","Max success rate : 0.2795\n","Avg success rate : 0.05464500000000001\n","\n","\n","Learning rate : 0.5\n","Max success rate : 0.593\n","Avg success rate : 0.07009000000000001\n","\n","\n","Learning rate : 0.6\n","Max success rate : 0.243\n","Avg success rate : 0.05355500000000001\n","\n","\n","Learning rate : 0.7\n","Max success rate : 0.1995\n","Avg success rate : 0.05374999999999999\n","\n","\n","Learning rate : 0.8\n","Max success rate : 0.3115\n","Avg success rate : 0.05796499999999999\n","\n","\n","Learning rate : 0.9\n","Max success rate : 0.24\n","Avg success rate : 0.045284999999999985\n","\n","\n","Learning rate : 1.0\n","Max success rate : 0.0345\n","Avg success rate : 0.020894999999999993\n","\n","\n"]}]},{"cell_type":"markdown","source":["- Q-learning에 learning rate와 discounted reword를 적용\n","- 별다른 차이가 없음\n"],"metadata":{"id":"tuEwIBjXuaQI"}},{"cell_type":"code","source":["NUM_EPISODES = 20000\n","\n","lr = 0.85\n","q_value, log_reword, log_q_map, log_action, log_q_by_step  = run_q_learning(env, NUM_EPISODES, learning_rate=lr)\n","res = sum(log_reword) / NUM_EPISODES\n","result_.append(res)\n","\n","print('Learning rate : ' + str(lr))\n","print('success rate : ' + str(res))\n","print('\\n')\n"],"metadata":{"id":"N0fGiKXQQjk5","executionInfo":{"status":"ok","timestamp":1651163110827,"user_tz":-540,"elapsed":5598,"user":{"displayName":"Jung Hyun Lee","userId":"01154420143394984910"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3197c92-3661-4ddb-bd79-9a039bccd63b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate : 0.85\n","success rate : 0.0174\n","\n","\n"]}]},{"cell_type":"code","source":["NUM_EPISODES = 200000\n","\n","lr = 0.85\n","q_value, log_reword, log_q_map, log_action, log_q_by_step  = run_q_learning(env, NUM_EPISODES, learning_rate=lr, discount=0.9)\n","res = sum(log_reword) / NUM_EPISODES\n","result_.append(res)\n","\n","print('Learning rate : ' + str(lr))\n","print('success rate : ' + str(res))\n","print('\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V1ZSjPOzGUwi","executionInfo":{"status":"ok","timestamp":1651165321727,"user_tz":-540,"elapsed":79461,"user":{"displayName":"Jung Hyun Lee","userId":"01154420143394984910"}},"outputId":"ca74fe6a-58f6-40b9-f640-a490a196ee6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate : 0.85\n","success rate : 0.026155\n","\n","\n"]}]},{"cell_type":"markdown","source":["- Episodes 횟수를 높여도 성공률이 낮음\n","- Learning rate 또는 discounted reward 적용하였을때 성공률이 높아지기는 했지만 일정이상 성공률이 높아지지는 않음\n","\n","\n"],"metadata":{"id":"PZVwtOZD4qJE"}}]}