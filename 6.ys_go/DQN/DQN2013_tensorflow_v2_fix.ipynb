{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN2013_tensorflow_v2_fix.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D_yCKEFMycnq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gym\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from gym.envs.registration import register\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 미끄러지지 않는 환경을 세팅합니다."
      ],
      "metadata": {
        "id": "n0n4XS10410_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "register(\n",
        "    id='LakeEnv-v1',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '4x4', 'is_slippery': False}\n",
        ")\n",
        "env = gym.make('LakeEnv-v1')"
      ],
      "metadata": {
        "id": "WjBvHvgO4V_G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 해당 state에 대한 정보를 받아오기 위한 과정"
      ],
      "metadata": {
        "id": "t-muZrCU4D1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# target 임베딩, input shape -> (1, 16)\n",
        "# state를 preprocessed sequenced, state를 우리가 원하는 형태로 바꾸는 함수\n",
        "def one_hot(x):\n",
        "    return np.identity(16)[x:x + 1].astype(np.float32)"
      ],
      "metadata": {
        "id": "rs5hUXLHv_bl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set DQN parameters\n",
        "num_episodes = 20000\n",
        "learning_rate = 0.1 \n",
        "dis = .99     # discount_reward\n",
        "h_size = 10    # 은닉층의 shape -> input(1,16) * hidden1(16,10) -> hidden2(10,4) -> output(1,4)\n",
        "\n",
        "REPLAY_MEMORY = 2500   # 버퍼의 최대 크기\n",
        "\n",
        "# buffer parameters\n",
        "random_sample = 10    # 버퍼가 다 찬 후 num_episodes에서 언제 random.sample을 할지 결정하는 파라미터 \n",
        "num_sample = 50   #sample을 뽑는 것을 얼마나 반복할 것인지 결정하는 파라미터\n",
        "num_choice = 10   # buffer에서 몇 개를 추출할 것인지 결정하는 파라미터\n",
        "\n",
        "# Input and output size based on the Env\n",
        "input_size = env.observation_space.n\n",
        "output_size = env.action_space.n\n",
        "\n",
        "# 업데이트되는 W를 통해 현재 state를 계산합니다.\n",
        "# weight 1  (16, 10)\n",
        "W1 = tf.Variable(tf.random.uniform([input_size, h_size], 0, 0.01), dtype=tf.float32)\n",
        "\n",
        "# weight 2  (10, 4)\n",
        "W2 = tf.Variable(tf.random.uniform([h_size, output_size], 0, 0.01), dtype=tf.float32)\n",
        "\n",
        "# optimizer\n",
        "optimizer = tf.optimizers.SGD(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "NJYY4uKb8MGr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# rewards per episode\n",
        "rList = []\n",
        "buffer = deque(maxlen=REPLAY_MEMORY)\n",
        "save_weight = []\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    # Reset environment and get first new observation\n",
        "    state = env.reset()\n",
        "    rAll = 0\n",
        "    step_count = 0\n",
        "    done = False\n",
        "    \n",
        "    e = 1. / ((i / 50) + 10)\n",
        "    # The Q-Table learning algorithm\n",
        "    while not done:\n",
        "        # Choose an action by greedly (with a chance of random action) from the Q-network\n",
        "        dense_layer_1 = tf.matmul(one_hot(state), W1)\n",
        "        activation_func_1 = tf.nn.relu(dense_layer_1)\n",
        "        dense_layer_2 = tf.matmul(activation_func_1, W2)\n",
        "        q_value = np.array(dense_layer_2.numpy())\n",
        "\n",
        "        if np.random.rand(1) < e:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_value)\n",
        "\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        # gym ai가 action을 취하고 얻은 정보를 학습시키지않고 buffer에 담습니다. \n",
        "        buffer.append((state, action, reward, state_next, done))\n",
        "        # 기존에 버퍼 크기를 지정\n",
        "        state = state_next\n",
        "        step_count += 1\n",
        "        if step_count > 100:\n",
        "          break\n",
        "\n",
        "    # buffer를 통한 학습 구현\n",
        "    # -----------------------------------------------------------------------------------------\n",
        "    if len(buffer) < num_choice:\n",
        "        continue\n",
        "    if i % random_sample == 1:\n",
        "        for _ in range(num_sample):\n",
        "            # buffer에서 학습할 데이터를 랜덤하게 뽑아서 학습합니다.\n",
        "            sample = random.sample(buffer, num_choice)\n",
        "\n",
        "            # sample의 개수 많큼 for문을 돌립니다\n",
        "            for state, action, reward, state_next, done in sample:\n",
        "                x_stack = np.empty(0, dtype=np.float32).reshape(0, input_size)\n",
        "                y_stack = np.empty(0, dtype=np.float32).reshape(0, output_size)\n",
        "                # 타겟값 구하기\n",
        "                if done:\n",
        "                    q_value[0, action] = reward\n",
        "                else:\n",
        "                    next_dense_layer_1 = tf.matmul(one_hot(state_next), W1)\n",
        "                    activation_func_1 = tf.nn.relu(next_dense_layer_1)\n",
        "\n",
        "                    next_dense_layer_2 = tf.matmul(activation_func_1, W2)\n",
        "                    q_score_next = np.array(next_dense_layer_2.numpy())\n",
        "\n",
        "                    q_value[0, action] = reward + dis * np.max(q_score_next)\n",
        "            \n",
        "                # Qpred_dense_layer_1 = tf.matmul(one_hot(state), W1)\n",
        "                # Qpred_activation_1 = tf.nn.relu(Qpred_dense_layer_1)\n",
        "                # Qpred_dense_layer_2 = tf.matmul(Qpred_activation_1, W2)\n",
        "                y_stack = np.vstack([y_stack, q_value])\n",
        "                x_stack = np.vstack([x_stack, one_hot(state)])\n",
        "\n",
        "            loss = lambda: tf.reduce_sum(input_tensor = (tf.square(y_stack - tf.matmul(tf.nn.relu(tf.matmul(x_stack, W1)), W2))))                              \n",
        "                                            \n",
        "            # optimizer, loss가 작아지는 방향으로 W 업데이트\n",
        "            optimizer.minimize(loss, var_list=[W1, W2])\n",
        "    # -----------------------------------------------------------------------------------------\n",
        "    if i % 100 == 0:\n",
        "        print(f'진행률: {round(i / num_episodes, 2) * 100}%')"
      ],
      "metadata": {
        "id": "oLmOdDqkyhzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEST"
      ],
      "metadata": {
        "id": "hDGyyUiepKi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "reward_sum = 0\n",
        "while True:\n",
        "    dense_layer_1 = tf.matmul(one_hot(state), W1)\n",
        "    activation_func_1 = tf.nn.relu(dense_layer_1)\n",
        "    dense_layer_2 = tf.matmul(activation_func_1, W2)\n",
        "    q_value = np.array(dense_layer_2.numpy())\n",
        "    \n",
        "    action = np.argmax(q_value)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    reward_sum += reward\n",
        "    if done:\n",
        "        print(f\"Total score: {reward_sum}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "mZKSmPiRo_im",
        "outputId": "6fd51a58-8819-4d11-ee30-369579abdeb3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-970563e2fd77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdense_layer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mactivation_func_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_layer_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdense_layer_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_func_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_layer_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3712\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3713\u001b[0m         return gen_math_ops.mat_mul(\n\u001b[0;32m-> 3714\u001b[0;31m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   3715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6013\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   6014\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6015\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   6016\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6017\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "제대로 학습이 이루어지지 않으면 길을 못찾는다."
      ],
      "metadata": {
        "id": "aZORMNeSrUpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결론\n",
        "\n",
        "DQN 2013은 기존에 Q-Network가 가지고 있는 연속된 데이터간의 유사성으로 인한 학습 능력 저하 문제를 데이터를 바로 학습에 사용하지않고 buffer를 통해 랜덤으로 추출하는 방법을 사용해서 해결했습니다. \n",
        "\n",
        "하지만 DQN 2013에서는 타겟값과 예측값이 같은 가중치 (W1, W2)를 공유해서 성능이 좋지않았습니다."
      ],
      "metadata": {
        "id": "iZThPS3h-5xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## error 코드 문제점 및 수정사항\n",
        "```python\n",
        "for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    rAll = 0\n",
        "    done = False\n",
        "    \n",
        "    e = 1. / ((i / 50) + 10)\n",
        "\n",
        "    while not done:\n",
        "        dense_layer_1 = tf.matmul(one_hot(state), W1)\n",
        "        activation_func = tf.nn.relu(dense_layer_1)\n",
        "\n",
        "        dense_layer_2 = tf.matmul(activation_func, W2)\n",
        "        activation_func = tf.nn.relu(dense_layer_2)\n",
        "\n",
        "        q_value = np.array(activation_func.numpy())\n",
        "\n",
        "        if np.random.rand(1) < e:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_value)\n",
        "\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        buffer.append((state, action, reward, state_next, done))\n",
        "        state = state_next\n",
        "\n",
        "        if len(buffer) > deque_maxlen:\n",
        "            pass\n",
        "\n",
        "        if num_episodes % random_sample == 1:\n",
        "            for _ in range(num_sample):\n",
        "                sample = random.sample(buffer, num_choice)\n",
        "\n",
        "                for state, action, reward, state_next, done in sample:\n",
        "                    if done:\n",
        "                        q_value[0, action] = reward\n",
        "                    else:\n",
        "                        dense_layer_1 = tf.matmul(one_hot(state_next), W1)\n",
        "                        activation_func = tf.nn.relu(dense_layer_1)\n",
        "\n",
        "                        dense_layer_2 = tf.matmul(activation_func, W2)\n",
        "                        activation_func = tf.nn.relu(dense_layer_2)\n",
        "                        \n",
        "                        q_score_next = np.array(activation_func.numpy())\n",
        "                        q_value[0, action] = reward + dis * np.max(q_score_next)\n",
        "\n",
        "                dense_layer_1 = tf.matmul(one_hot(state), W1)\n",
        "                activation_func = tf.nn.relu(dense_layer_1)\n",
        "\n",
        "                loss = lambda: tf.reduce_sum(input_tensor=tf.square(q_value - tf.nn.relu(tf.matmul(activation_func, W2))))\n",
        "                optimizer.minimize(loss, var_list=[W1, W2])\n",
        "```\n",
        "<br>\n",
        "\n",
        "1. 여기서 num_episode는 에피소드를 얼마나 진행할지 정하는 파라미터이고, 우리가 하고자 하는 것은 에피소드를 진행하면서 에피소드가 10이 될 때마다 랜덤해서 추출하는 것이기 때문에 num_episodes가 i로 바꿔야합니다.\n",
        "\n",
        "2. while not done 루프를 통해 우리가 얻는 것은 버퍼에 들어가는 데이터고 우리는 바로 학습하는 것이 아니라 버퍼에서 데이터를 랜덤하게 추출해서 학습하는 것이므로 while문 안에서 들어가면 안되고 에피소드를 진행하는 for문에 안에 있어야합니다.\n",
        "\n",
        "3. while문이 끝나지 않는 문제가 있었습니다. action을 버퍼에 담아서 확인해본 결과, agent가 목적지를 찾기 위해 action을 진행하면서 계속 같은 자리에서 머무는 경우가 있었습니다. 그래서 이 문제를 해결하기위해서 step제한을 넣어줬습니다.\n",
        "\n",
        "4. 버퍼에 충분한 데이터가 담기지 않아서 랜덤샘플시 데이터가 샘플로 추출할 개수보다 부족하다는 오류가 떠서 continue를 사용해 버퍼에 데이터가 샘플링할 데이터보다 작을 경우 건너뛰도록 했습니다.\n",
        "\n",
        "5. error code에서 1번 문제 때문에 학습부분이 실행되지 않아서 오류로 뜨지않았는데 loss 계산부분에서 W1, W2 값 두개를 바꿔주는데 loss식 안에 W2밖에 존재하지 않아서 오류가 떴습니다. 이 문제를 해결하기위해 네트워크를 통해 예측값을 구하는 부분을 한 줄로 풀어써줬습니다.\n",
        "\n",
        "6. 샘플링을 통해 학습할 때마다 optimizer를 사용했을 때 학습시간이 너무 길어서 DQN 강의 1을 보고 np.vstack을 사용한 결과 좀 더 빠르게 학습을 할 수 있었습니다. 강의에서는 state를 넣어 줬는데 state는 숫자 하나이고 x_stack은 [0,16] 벡터라 결합이 되지 않았습니다. 그래서 원핫인코딩을 사용해서 해결했습니다.\n",
        "\n",
        "7. cannot compute MatMul as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:MatMul] 오류가 발생했습니다. np.empty를 생성할 때 dtype을 맞춰주지않아서 생긴 오류입니다. \n",
        "\n"
      ],
      "metadata": {
        "id": "gDW3YIGYdWU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN 과정"
      ],
      "metadata": {
        "id": "LpnyY6g3kwYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 네트워크를 만들고 안에 있는 가중치값들을 초기화 합니다..\n",
        "2. 환경 만들기\n",
        "3. 루프 시작\n",
        "\n",
        "  - E-greedy방법을 통해 action을 랜덤하게 선택하거나, state를 넣어 네트워크를 통해 action을 받아옵니다.\n",
        "  - env.step을 통해 action을 취해서 얻는 state_next, reward, done 정보와 state, action 을 바로 학습하지않고 버퍼에 저장합니다.\n",
        "  - 루프에 10번에 1번마다 버퍼에서 랜덤하게 데이터를 샘플해서 네트워크를 학습합니다. (현재 코드)\n"
      ],
      "metadata": {
        "id": "QVec53Bsar5D"
      }
    }
  ]
}